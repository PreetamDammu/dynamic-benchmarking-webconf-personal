@misc{bangHalluLensLLMHallucination2025,
  title = {{{HalluLens}}: {{LLM Hallucination Benchmark}}},
  shorttitle = {{{HalluLens}}},
  author = {Bang, Yejin and Ji, Ziwei and Schelten, Alan and Hartshorn, Anthony and Fowler, Tara and Zhang, Cheng and Cancedda, Nicola and Fung, Pascale},
  year = {2025},
  month = apr,
  number = {arXiv:2504.17550},
  eprint = {2504.17550},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.17550},
  urldate = {2025-09-26},
  abstract = {Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: 42 pages},
  file = {/Users/preetams/Zotero/storage/TZ5XK539/Bang et al. - 2025 - HalluLens LLM Hallucination Benchmark.pdf;/Users/preetams/Zotero/storage/75S4H383/2504.html}
}

@misc{carliniQuantifyingMemorizationNeural2023,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = {2023},
  month = mar,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2202.07646},
  urldate = {2025-09-26},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/preetams/Zotero/storage/8I6HM9XT/Carlini et al. - 2023 - Quantifying Memorization Across Neural Language Models.pdf;/Users/preetams/Zotero/storage/W9SGZEEM/2202.html}
}

@misc{carragherQuantifyingMemorizationRetriever2025,
  title = {Quantifying {{Memorization}} and {{Retriever Performance}} in {{Retrieval-Augmented Vision-Language Models}}},
  author = {Carragher, Peter and Jha, Abhinand and Raghav, R. and Carley, Kathleen M.},
  year = {2025},
  month = feb,
  number = {arXiv:2502.13836},
  eprint = {2502.13836},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13836},
  urldate = {2025-09-26},
  abstract = {Large Language Models (LLMs) demonstrate remarkable capabilities in question answering (QA), but metrics for assessing their reliance on memorization versus retrieval remain underdeveloped. Moreover, while finetuned models are state-of-the-art on closed-domain tasks, general-purpose models like GPT-4o exhibit strong zero-shot performance. This raises questions about the trade-offs between memorization, generalization, and retrieval. In this work, we analyze the extent to which multimodal retrieval-augmented VLMs memorize training data compared to baseline VLMs. Using the WebQA benchmark, we contrast finetuned models with baseline VLMs on multihop retrieval and question answering, examining the impact of finetuning on data memorization. To quantify memorization in end-to-end retrieval and QA systems, we propose several proxy metrics by investigating instances where QA succeeds despite retrieval failing. Our results reveal the extent to which finetuned models rely on memorization. In contrast, retrieval-augmented VLMs have lower memorization scores, at the cost of accuracy (72\% vs 52\% on WebQA test set). As such, our measures pose a challenge for future work to reconcile memorization and generalization in both Open-Domain QA and joint Retrieval-QA tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/preetams/Zotero/storage/UZJWVIE9/Carragher et al. - 2025 - Quantifying Memorization and Retriever Performance in Retrieval-Augmented Vision-Language Models.pdf;/Users/preetams/Zotero/storage/TJN6F7E6/2502.html}
}

@misc{chenDatasetAnsweringTimeSensitive2021,
  title = {A {{Dataset}} for {{Answering Time-Sensitive Questions}}},
  author = {Chen, Wenhu and Wang, Xinyi and Wang, William Yang},
  year = {2021},
  month = oct,
  number = {arXiv:2108.06314},
  eprint = {2108.06314},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.06314},
  urldate = {2025-09-26},
  abstract = {Time is an important dimension in our physical world. Lots of facts can evolve with respect to time. For example, the U.S. President might change every four years. Therefore, it is important to consider the time dimension and empower the existing QA models to reason over time. However, the existing QA datasets contain rather few time-sensitive questions, hence not suitable for diagnosing or benchmarking the model's temporal reasoning capability. In order to promote research in this direction, we propose to construct a time-sensitive QA dataset. The dataset is constructed by 1) mining time-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, 2) employing crowd workers to verify and calibrate these noisy facts, 3) generating question-answer pairs based on the annotated time-sensitive facts. Our dataset poses challenges in the aspect of both temporal understanding and temporal reasoning. We evaluate different SoTA long-document QA systems like BigBird and FiD on our dataset. The best-performing model FiD can only achieve 46{\textbackslash}\% accuracy, still far behind the human performance of 87{\textbackslash}\%. We demonstrate that these models are still lacking the ability to perform consistent temporal reasoning. Therefore, we believe that our dataset could serve as a benchmark to develop NLP models more sensitive to temporal shifts. The dataset and code are released in{\textasciitilde}{\textbackslash}url\{https://github.com/wenhuchen/Time-Sensitive-QA\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: Accepted to NeurIPS21 (dataset track), it contains 10 pages of main text and appendix},
  file = {/Users/preetams/Zotero/storage/5VHJCGUT/Chen et al. - 2021 - A Dataset for Answering Time-Sensitive Questions.pdf;/Users/preetams/Zotero/storage/EC28RB22/2108.html}
}

@misc{chenReadingWikipediaAnswer2017a,
  title = {Reading {{Wikipedia}} to {{Answer Open-Domain Questions}}},
  author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  year = {2017},
  month = apr,
  number = {arXiv:1704.00051},
  eprint = {1704.00051},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1704.00051},
  urldate = {2025-09-26},
  abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: ACL2017, 10 pages},
  file = {/Users/preetams/Zotero/storage/CTMHK8ZM/Chen et al. - 2017 - Reading Wikipedia to Answer Open-Domain Questions.pdf;/Users/preetams/Zotero/storage/QJHYY6A4/1704.html}
}

@inproceedings{dammuDynamicKGQAScalableFramework2025a,
  title = {Dynamic-{{KGQA}}: {{A Scalable Framework}} for {{Generating Adaptive Question Answering Datasets}}},
  shorttitle = {Dynamic-{{KGQA}}},
  booktitle = {Proceedings of the 48th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Dammu, Preetam Prabhu Srikar and Naidu, Himanshu and Shah, Chirag},
  year = {2025},
  month = jul,
  series = {{{SIGIR}} '25},
  pages = {3498--3508},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3726302.3730324},
  urldate = {2025-09-26},
  abstract = {As question answering (QA) systems advance alongside the rapid evolution of foundation models, the need for robust, adaptable, and large-scale evaluation benchmarks becomes increasingly critical. Traditional QA benchmarks are often static and publicly available, making them susceptible to data contamination and memorization by large language models (LLMs). Consequently, static benchmarks may overestimate model generalization and hinder a reliable assessment of real-world performance. In this work, we introduce Dynamic-KGQA, a scalable framework for generating adaptive QA datasets from knowledge graphs (KGs), designed to mitigate memorization risks while maintaining statistical consistency across iterations. Unlike fixed benchmarks, Dynamic-KGQA generates a new dataset variant on every run while preserving the underlying distribution, enabling fair and reproducible evaluations. Furthermore, our framework provides fine-grained control over dataset characteristics, supporting domain-specific and topic-focused QA dataset generation. Additionally, Dynamic-KGQA produces compact, semantically coherent subgraphs that facilitate both training and evaluation of KGQA models, enhancing their ability to leverage structured knowledge effectively. To align with existing evaluation protocols, we also provide static large-scale train/test/validation splits, ensuring comparability with prior methods. By introducing a dynamic, customizable benchmarking paradigm, Dynamic-KGQA enables a more rigorous and adaptable evaluation of QA systems.},
  isbn = {979-8-4007-1592-1},
  file = {/Users/preetams/Zotero/storage/V2AQHXLU/Dammu et al. - 2025 - Dynamic-KGQA A Scalable Framework for Generating Adaptive Question Answering Datasets.pdf}
}

@misc{izacardLeveragingPassageRetrieval2021,
  title = {Leveraging {{Passage Retrieval}} with {{Generative Models}} for {{Open Domain Question Answering}}},
  author = {Izacard, Gautier and Grave, Edouard},
  year = {2021},
  month = feb,
  number = {arXiv:2007.01282},
  eprint = {2007.01282},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2007.01282},
  urldate = {2025-09-26},
  abstract = {Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/preetams/Zotero/storage/TPEAIN8U/Izacard and Grave - 2021 - Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering.pdf;/Users/preetams/Zotero/storage/UVQJD6R7/2007.html}
}

@misc{jiaAdversarialExamplesEvaluating2017,
  title = {Adversarial {{Examples}} for {{Evaluating Reading Comprehension Systems}}},
  author = {Jia, Robin and Liang, Percy},
  year = {2017},
  month = jul,
  number = {arXiv:1707.07328},
  eprint = {1707.07328},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.07328},
  urldate = {2025-09-26},
  abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of \$75{\textbackslash}\%\$ F1 score to \$36{\textbackslash}\%\$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to \$7{\textbackslash}\%\$. We hope our insights will motivate the development of new models that understand language more precisely.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: EMNLP 2017},
  file = {/Users/preetams/Zotero/storage/UKAN7TCH/Jia and Liang - 2017 - Adversarial Examples for Evaluating Reading Comprehension Systems.pdf;/Users/preetams/Zotero/storage/9WSPGEEB/1707.html}
}

@misc{joshiTriviaQALargeScale2017,
  title = {{{TriviaQA}}: {{A Large Scale Distantly Supervised Challenge Dataset}} for {{Reading Comprehension}}},
  shorttitle = {{{TriviaQA}}},
  author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
  year = {2017},
  month = may,
  number = {arXiv:1705.03551},
  eprint = {1705.03551},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1705.03551},
  urldate = {2025-09-26},
  abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Added references, fixed typos, minor baseline update},
  file = {/Users/preetams/Zotero/storage/LDMSS2GB/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.pdf;/Users/preetams/Zotero/storage/WJQ3YIJA/1705.html}
}

@misc{kasaiRealTimeQAWhats2024,
  title = {{{RealTime QA}}: {{What}}'s the {{Answer Right Now}}?},
  shorttitle = {{{RealTime QA}}},
  author = {Kasai, Jungo and Sakaguchi, Keisuke and Takahashi, Yoichi and Bras, Ronan Le and Asai, Akari and Yu, Xinyan and Radev, Dragomir and Smith, Noah A. and Choi, Yejin and Inui, Kentaro},
  year = {2024},
  month = feb,
  number = {arXiv:2207.13332},
  eprint = {2207.13332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.13332},
  urldate = {2025-09-26},
  abstract = {We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will spur progress in instantaneous applications of question answering and beyond.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: RealTime QA Website: https://realtimeqa.github.io/},
  file = {/Users/preetams/Zotero/storage/LWVWTZBX/Kasai et al. - 2024 - RealTime QA What's the Answer Right Now.pdf;/Users/preetams/Zotero/storage/RW3NGZVQ/2207.html}
}

@misc{kielaDynabenchRethinkingBenchmarking2021a,
  title = {Dynabench: {{Rethinking Benchmarking}} in {{NLP}}},
  shorttitle = {Dynabench},
  author = {Kiela, Douwe and Bartolo, Max and Nie, Yixin and Kaushik, Divyansh and Geiger, Atticus and Wu, Zhengxuan and Vidgen, Bertie and Prasad, Grusha and Singh, Amanpreet and Ringshia, Pratik and Ma, Zhiyi and Thrush, Tristan and Riedel, Sebastian and Waseem, Zeerak and Stenetorp, Pontus and Jia, Robin and Bansal, Mohit and Potts, Christopher and Williams, Adina},
  year = {2021},
  month = apr,
  number = {arXiv:2104.14337},
  eprint = {2104.14337},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.14337},
  urldate = {2025-09-26},
  abstract = {We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: NAACL 2021},
  file = {/Users/preetams/Zotero/storage/LFXXCEY3/Kiela et al. - 2021 - Dynabench Rethinking Benchmarking in NLP.pdf;/Users/preetams/Zotero/storage/V4YQ7JJP/2104.html}
}

@misc{kimQA$^2$QuestionAnswering2023,
  title = {({{QA}})\${\textasciicircum}2\$: {{Question Answering}} with {{Questionable Assumptions}}},
  shorttitle = {({{QA}})\${\textasciicircum}2\$},
  author = {Kim, Najoung and Htut, Phu Mon and Bowman, Samuel R. and Petty, Jackson},
  year = {2023},
  month = aug,
  number = {arXiv:2212.10003},
  eprint = {2212.10003},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10003},
  urldate = {2025-09-26},
  abstract = {Naturally occurring information-seeking questions often contain questionable assumptions -- assumptions that are false or unverifiable. Questions containing questionable assumptions are challenging because they require a distinct answer strategy that deviates from typical answers for information-seeking questions. For instance, the question "When did Marie Curie discover Uranium?" cannot be answered as a typical "when" question without addressing the false assumption "Marie Curie discovered Uranium". In this work, we propose (QA)\${\textasciicircum}2\$ (Question Answering with Questionable Assumptions), an open-domain evaluation dataset consisting of naturally occurring search engine queries that may or may not contain questionable assumptions. To be successful on (QA)\${\textasciicircum}2\$, systems must be able to detect questionable assumptions and also be able to produce adequate responses for both typical information-seeking questions and ones with questionable assumptions. Through human rater acceptability on end-to-end QA with (QA)\${\textasciicircum}2\$, we find that current models do struggle with handling questionable assumptions, leaving substantial headroom for progress.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: ACL 2023 camera-ready},
  file = {/Users/preetams/Zotero/storage/KBSHUR9T/Kim et al. - 2023 - (QA)$^2$ Question Answering with Questionable Assumptions.pdf;/Users/preetams/Zotero/storage/UEM2LTTI/2212.html}
}

@article{kwiatkowskiNaturalQuestionsBenchmark2019a,
  title = {Natural {{Questions}}: {{A Benchmark}} for {{Question Answering Research}}},
  shorttitle = {Natural {{Questions}}},
  author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year = {2019},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {452--466},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00276},
  urldate = {2025-09-26},
  abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  file = {/Users/preetams/Zotero/storage/CHBHYBZ6/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answering Research.pdf}
}

@misc{liOpenSourceData2024,
  title = {An {{Open Source Data Contamination Report}} for {{Large Language Models}}},
  author = {Li, Yucheng and Guerin, Frank and Lin, Chenghua},
  year = {2024},
  month = jan,
  number = {arXiv:2310.17589},
  eprint = {2310.17589},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17589},
  urldate = {2025-09-26},
  abstract = {Data contamination in model evaluation has become increasingly prevalent with the growing popularity of large language models. It allows models to "cheat" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has become an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by large language model developers and often lacks transparency and completeness. This paper presents an extensive data contamination report for over 15 popular large language models across six popular multiple-choice QA benchmarks. We also introduce an open-source pipeline that enables the community to perform contamination analysis on customised data and models. Our experiments reveal varying contamination levels ranging from 1{\textbackslash}\% to 45{\textbackslash}\% across benchmarks, with the contamination degree increasing rapidly over time. Performance analysis of large language models indicates that data contamination does not necessarily lead to increased model metrics: while significant accuracy boosts of up to 14{\textbackslash}\% and 7{\textbackslash}\% are observed on contaminated C-Eval and Hellaswag benchmarks, only a minimal increase is noted on contaminated MMLU. We also find larger models seem able to gain more advantages than smaller models on contaminated test sets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/EWSP3M7Y/Li et al. - 2024 - An Open Source Data Contamination Report for Large Language Models.pdf;/Users/preetams/Zotero/storage/MZC38GZK/2310.html}
}

@misc{liskaStreamingQABenchmarkAdaptation2022,
  title = {{{StreamingQA}}: {{A Benchmark}} for {{Adaptation}} to {{New Knowledge}} over {{Time}} in {{Question Answering Models}}},
  shorttitle = {{{StreamingQA}}},
  author = {Li{\v s}ka, Adam and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and {d'Autume}, Cyprien de Masson and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and {Gilsenan-McMahon}, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  year = {2022},
  month = may,
  number = {arXiv:2205.11388},
  eprint = {2205.11388},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.11388},
  urldate = {2025-09-26},
  abstract = {Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models' knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/preetams/Zotero/storage/7J2DCUNX/Liška et al. - 2022 - StreamingQA A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models.pdf;/Users/preetams/Zotero/storage/BMJBR97G/2205.html}
}

@misc{liuLanguageModelsMay2025,
  title = {Language {{Models May Verbatim Complete Text They Were Not Explicitly Trained On}}},
  author = {Liu, Ken Ziyu and {Choquette-Choo}, Christopher A. and Jagielski, Matthew and Kairouz, Peter and Koyejo, Sanmi and Liang, Percy and Papernot, Nicolas},
  year = {2025},
  month = mar,
  number = {arXiv:2503.17514},
  eprint = {2503.17514},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.17514},
  urldate = {2025-09-26},
  abstract = {An important question today is whether a given text was used to train a large language model (LLM). A {\textbackslash}emph\{completion\} test is often employed: check if the LLM completes a sufficiently complex text. This, however, requires a ground-truth definition of membership; most commonly, it is defined as a member based on the \$n\$-gram overlap between the target text and any text in the dataset. In this work, we demonstrate that this \$n\$-gram based membership definition can be effectively gamed. We study scenarios where sequences are {\textbackslash}emph\{non-members\} for a given \$n\$ and we find that completion tests still succeed. We find many natural cases of this phenomenon by retraining LLMs from scratch after removing all training samples that were completed; these cases include exact duplicates, near-duplicates, and even short overlaps. They showcase that it is difficult to find a single viable choice of \$n\$ for membership definitions. Using these insights, we design adversarial datasets that can cause a given target sequence to be completed without containing it, for any reasonable choice of \$n\$. Our findings highlight the inadequacy of \$n\$-gram membership, suggesting membership definitions fail to account for auxiliary information available to the training algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  note = {Comment: Main text: 9 pages, 7 figures, 1 table. Appendix: 29 pages, 20 tables, 15 figures},
  file = {/Users/preetams/Zotero/storage/VS665U5A/Liu et al. - 2025 - Language Models May Verbatim Complete Text They Were Not Explicitly Trained On.pdf;/Users/preetams/Zotero/storage/33QEZMAR/2503.html}
}

@misc{liuOpenDomainQuestion2025,
  title = {Open {{Domain Question Answering}} with {{Conflicting Contexts}}},
  author = {Liu, Siyi and Ning, Qiang and Halder, Kishaloy and Xiao, Wei and Qi, Zheng and Htut, Phu Mon and Zhang, Yi and John, Neha Anna and Min, Bonan and Benajiba, Yassine and Roth, Dan},
  year = {2025},
  month = apr,
  number = {arXiv:2410.12311},
  eprint = {2410.12311},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.12311},
  urldate = {2025-09-26},
  abstract = {Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25\% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/MZ5JCIN2/Liu et al. - 2025 - Open Domain Question Answering with Conflicting Contexts.pdf;/Users/preetams/Zotero/storage/4FEW93YB/2410.html}
}

@misc{maDynaboardEvaluationAsAServicePlatform2021,
  title = {Dynaboard: {{An Evaluation-As-A-Service Platform}} for {{Holistic Next-Generation Benchmarking}}},
  shorttitle = {Dynaboard},
  author = {Ma, Zhiyi and Ethayarajh, Kawin and Thrush, Tristan and Jain, Somya and Wu, Ledell and Jia, Robin and Potts, Christopher and Williams, Adina and Kiela, Douwe},
  year = {2021},
  month = may,
  number = {arXiv:2106.06052},
  eprint = {2106.06052},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06052},
  urldate = {2025-09-26},
  abstract = {We introduce Dynaboard, an evaluation-as-a-service framework for hosting benchmarks and conducting holistic model comparison, integrated with the Dynabench platform. Our platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. Under this paradigm, models are submitted to be evaluated in the cloud, circumventing the issues of reproducibility, accessibility, and backwards compatibility that often hinder benchmarking in NLP. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which -- despite their importance to practitioners -- have traditionally been absent from leaderboards. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reflect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/7X3U7K7J/Ma et al. - 2021 - Dynaboard An Evaluation-As-A-Service Platform for Holistic Next-Generation Benchmarking.pdf;/Users/preetams/Zotero/storage/G8FJDGBB/2106.html}
}

@misc{morrisHowMuchLanguage2025,
  title = {How Much Do Language Models Memorize?},
  author = {Morris, John X. and Sitawarin, Chawin and Guo, Chuan and Kokhlikyan, Narine and Suh, G. Edward and Rush, Alexander M. and Chaudhuri, Kamalika and Mahloujifar, Saeed},
  year = {2025},
  month = may,
  number = {arXiv:2505.24832},
  eprint = {2505.24832},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.24832},
  urldate = {2025-09-26},
  abstract = {We propose a new method for estimating how much a model ``knows'' about a datapoint and use it to measure the capacity of modern language models. Prior studies of language model memorization have struggled to disentangle memorization from generalization. We formally separate memorization into two components: {\textbackslash}textit\{unintended memorization\}, the information a model contains about a specific dataset, and {\textbackslash}textit\{generalization\}, the information a model contains about the true data-generation process. When we completely eliminate generalization, we can compute the total memorization, which provides an estimate of model capacity: our measurements estimate that GPT-style models have a capacity of approximately 3.6 bits per parameter. We train language models on datasets of increasing size and observe that models memorize until their capacity fills, at which point ``grokking'' begins, and unintended memorization decreases as models begin to generalize. We train hundreds of transformer language models ranging from \$500K\$ to \$1.5B\$ parameters and produce a series of scaling laws relating model capacity and data size to membership inference.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/BA9WGV74/Morris et al. - 2025 - How much do language models memorize.pdf;/Users/preetams/Zotero/storage/TJDUYNUW/2505.html}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022a,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  year = {2022},
  month = jun,
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.09332},
  urldate = {2025-09-26},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 32 pages},
  file = {/Users/preetams/Zotero/storage/JCLXWFGC/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with human feedback.pdf;/Users/preetams/Zotero/storage/3YCPG9IV/2112.html}
}

@misc{pengLimitationsTransformerArchitecture2024,
  title = {On {{Limitations}} of the {{Transformer Architecture}}},
  author = {Peng, Binghui and Narayanan, Srini and Papadimitriou, Christos},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08164},
  eprint = {2402.08164},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.08164},
  urldate = {2025-09-26},
  abstract = {What are the root causes of hallucinations in large language models (LLMs)? We use Communication Complexity to prove that the Transformer layer is incapable of composing functions (e.g., identify a grandparent of a person in a genealogy) if the domains of the functions are large enough; we show through examples that this inability is already empirically present when the domains are quite small. We also point out that several mathematical tasks that are at the core of the so-called compositional tasks thought to be hard for LLMs are unlikely to be solvable by Transformers, for large enough instances and assuming that certain well accepted conjectures in the field of Computational Complexity are true.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/preetams/Zotero/storage/UWJ9ATAA/Peng et al. - 2024 - On Limitations of the Transformer Architecture.pdf;/Users/preetams/Zotero/storage/8J2Z4BBC/2402.html}
}

@misc{ravautComprehensiveSurveyContamination2025a,
  title = {A {{Comprehensive Survey}} of {{Contamination Detection Methods}} in {{Large Language Models}}},
  author = {Ravaut, Mathieu and Ding, Bosheng and Jiao, Fangkai and Chen, Hailin and Li, Xingxuan and Zhao, Ruochen and Qin, Chengwei and Xiong, Caiming and Joty, Shafiq},
  year = {2025},
  month = jul,
  number = {arXiv:2404.00699},
  eprint = {2404.00699},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.00699},
  urldate = {2025-09-26},
  abstract = {With the rise of Large Language Models (LLMs) in recent years, abundant new opportunities are emerging, but also new challenges, among which contamination is quickly becoming critical. Business applications and fundraising in Artificial Intelligence (AI) have reached a scale at which a few percentage points gained on popular question-answering benchmarks could translate into dozens of millions of dollars, placing high pressure on model integrity. At the same time, it is becoming harder and harder to keep track of the data that LLMs have seen; if not impossible with closed-source models like GPT-4 and Claude-3 not divulging any information on the training set. As a result, contamination becomes a major issue: LLMs' performance may not be reliable anymore, as the high performance may be at least partly due to their previous exposure to the data. This limitation jeopardizes real capability improvement in the field of NLP, yet, there remains a lack of methods on how to efficiently detect contamination. In this paper, we survey all recent work on contamination detection with LLMs, analyzing their methodologies and use cases to shed light on the appropriate usage of contamination detection methods. Our work calls the NLP research community's attention into systematically taking into account contamination bias in LLM evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted by TMLR in July 2025. 18 pages, 1 figure, 3 tables},
  file = {/Users/preetams/Zotero/storage/XCPWBQB7/Ravaut et al. - 2025 - A Comprehensive Survey of Contamination Detection Methods in Large Language Models.pdf;/Users/preetams/Zotero/storage/TV89PGYR/2404.html}
}

@misc{rawlesAndroidWorldDynamicBenchmarking2025,
  title = {{{AndroidWorld}}: {{A Dynamic Benchmarking Environment}} for {{Autonomous Agents}}},
  shorttitle = {{{AndroidWorld}}},
  author = {Rawles, Christopher and Clinckemaillie, Sarah and Chang, Yifan and Waltz, Jonathan and Lau, Gabrielle and Fair, Marybeth and Li, Alice and Bishop, William and Li, Wei and {Campbell-Ajala}, Folawiyo and Toyama, Daniel and Berry, Robert and Tyamagundlu, Divya and Lillicrap, Timothy and Riva, Oriana},
  year = {2025},
  month = apr,
  number = {arXiv:2405.14573},
  eprint = {2405.14573},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.14573},
  urldate = {2025-09-26},
  abstract = {Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device's system state. We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6\% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at github.com/google-research/android\_world.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/preetams/Zotero/storage/DCU5SIYX/Rawles et al. - 2025 - AndroidWorld A Dynamic Benchmarking Environment for Autonomous Agents.pdf;/Users/preetams/Zotero/storage/GYW74UVJ/2405.html}
}

@misc{suAPIEnoughConformal2024,
  title = {{{API Is Enough}}: {{Conformal Prediction}} for {{Large Language Models Without Logit-Access}}},
  shorttitle = {{{API Is Enough}}},
  author = {Su, Jiayuan and Luo, Jing and Wang, Hongwei and Cheng, Lu},
  year = {2024},
  month = apr,
  number = {arXiv:2403.01216},
  eprint = {2403.01216},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.01216},
  urldate = {2025-09-26},
  abstract = {This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on both close-ended and open-ended Question Answering tasks show our approach can mostly outperform the logit-based CP baselines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/preetams/Zotero/storage/9B36MK6D/Su et al. - 2024 - API Is Enough Conformal Prediction for Large Language Models Without Logit-Access.pdf;/Users/preetams/Zotero/storage/LPLGCSJQ/2403.html}
}

@misc{thakurBEIRHeterogenousBenchmark2021,
  title = {{{BEIR}}: {{A Heterogenous Benchmark}} for {{Zero-shot Evaluation}} of {{Information Retrieval Models}}},
  shorttitle = {{{BEIR}}},
  author = {Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  year = {2021},
  month = oct,
  number = {arXiv:2104.08663},
  eprint = {2104.08663},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08663},
  urldate = {2025-09-26},
  abstract = {Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction-based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  note = {Comment: Accepted at NeurIPS 2021 Dataset and Benchmark Track},
  file = {/Users/preetams/Zotero/storage/PYZHZU3E/Thakur et al. - 2021 - BEIR A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models.pdf;/Users/preetams/Zotero/storage/XD26N74Q/2104.html}
}

@misc{thrushDynataskFrameworkCreating2022,
  title = {Dynatask: {{A Framework}} for {{Creating Dynamic AI Benchmark Tasks}}},
  shorttitle = {Dynatask},
  author = {Thrush, Tristan and Tirumala, Kushal and Gupta, Anmol and Bartolo, Max and Rodriguez, Pedro and Kane, Tariq and Rojas, William Gaviria and Mattson, Peter and Williams, Adina and Kiela, Douwe},
  year = {2022},
  month = apr,
  number = {arXiv:2204.01906},
  eprint = {2204.01906},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2204.01906},
  urldate = {2025-09-26},
  abstract = {We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at https://dynabench.org/ and the full library can be found at https://github.com/facebookresearch/dynabench.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: ACL System Demos 2022},
  file = {/Users/preetams/Zotero/storage/QVYDUYGV/Thrush et al. - 2022 - Dynatask A Framework for Creating Dynamic AI Benchmark Tasks.pdf;/Users/preetams/Zotero/storage/DKYUM4UW/2204.html}
}

@misc{tirumalaMemorizationOverfittingAnalyzing2022,
  title = {Memorization {{Without Overfitting}}: {{Analyzing}} the {{Training Dynamics}} of {{Large Language Models}}},
  shorttitle = {Memorization {{Without Overfitting}}},
  author = {Tirumala, Kushal and Markosyan, Aram H. and Zettlemoyer, Luke and Aghajanyan, Armen},
  year = {2022},
  month = nov,
  number = {arXiv:2205.10770},
  eprint = {2205.10770},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.10770},
  urldate = {2025-09-26},
  abstract = {Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, finding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-fitting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and find that models memorize nouns and numbers first; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identifier for memorizing individual training examples. Together, these findings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/CHZJNKAT/Tirumala et al. - 2022 - Memorization Without Overfitting Analyzing the Training Dynamics of Large Language Models.pdf;/Users/preetams/Zotero/storage/IDI5S6CS/2205.html}
}

@misc{vuFreshLLMsRefreshingLarge2023,
  title = {{{FreshLLMs}}: {{Refreshing Large Language Models}} with {{Search Engine Augmentation}}},
  shorttitle = {{{FreshLLMs}}},
  author = {Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and Luong, Thang},
  year = {2023},
  month = nov,
  number = {arXiv:2310.03214},
  eprint = {2310.03214},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.03214},
  urldate = {2025-09-26},
  abstract = {Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Preprint, 26 pages, 10 figures, 5 tables; Added FreshEval},
  file = {/Users/preetams/Zotero/storage/2MRN3MA4/Vu et al. - 2023 - FreshLLMs Refreshing Large Language Models with Search Engine Augmentation.pdf;/Users/preetams/Zotero/storage/NQEWMY4Q/2310.html}
}

@misc{yangHotpotQADatasetDiverse2018a,
  title = {{{HotpotQA}}: {{A Dataset}} for {{Diverse}}, {{Explainable Multi-hop Question Answering}}},
  shorttitle = {{{HotpotQA}}},
  author = {Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
  year = {2018},
  month = sep,
  number = {arXiv:1809.09600},
  eprint = {1809.09600},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1809.09600},
  urldate = {2025-09-26},
  abstract = {Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: EMNLP 2018 long paper. The first three authors contribute equally. Data, code, and blog posts available at https://hotpotqa.github.io/},
  file = {/Users/preetams/Zotero/storage/54FX6HML/Yang et al. - 2018 - HotpotQA A Dataset for Diverse, Explainable Multi-hop Question Answering.pdf;/Users/preetams/Zotero/storage/K498W2VL/1809.html}
}

@misc{yangRethinkingBenchmarkContamination2023,
  title = {Rethinking {{Benchmark}} and {{Contamination}} for {{Language Models}} with {{Rephrased Samples}}},
  author = {Yang, Shuo and Chiang, Wei-Lin and Zheng, Lianmin and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04850},
  eprint = {2311.04850},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.04850},
  urldate = {2025-09-26},
  abstract = {Large language models are increasingly trained on all the data ever produced by humans. Many have raised concerns about the trustworthiness of public benchmarks due to potential contamination in pre-training or fine-tuning datasets. While most data decontamination efforts apply string matching (e.g., n-gram overlap) to remove benchmark data, we show that these methods are insufficient, and simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures. Furthermore, we demonstrate that if such variation of test data is not eliminated, a 13B model can easily overfit a test benchmark and achieve drastically high performance, on par with GPT-4. We validate such observations in widely used benchmarks such as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a stronger LLM-based decontamination method and apply it to widely used pre-training and fine-tuning datasets, revealing significant previously unknown test overlap. For example, in pre-training sets such as RedPajama-Data-1T and StarCoder-Data, we identified that 8-18{\textbackslash}\% of the HumanEval benchmark overlaps. Interestingly, we also find such contamination in synthetic dataset generated by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We urge the community to adopt stronger decontamination approaches when using public benchmarks. Moreover, we call for the community to actively develop fresh one-time exams to evaluate models accurately. Our decontamination tool is publicly available at https://github.com/lm-sys/llm-decontaminator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/CPTEJYYN/Yang et al. - 2023 - Rethinking Benchmark and Contamination for Language Models with Rephrased Samples.pdf;/Users/preetams/Zotero/storage/QU3DBB2S/2311.html}
}

@misc{yao$t$benchBenchmarkToolAgentUser2024,
  title = {\${$\tau\$$}-Bench: {{A Benchmark}} for {{Tool-Agent-User Interaction}} in {{Real-World Domains}}},
  shorttitle = {\${$\tau\$$}-Bench},
  author = {Yao, Shunyu and Shinn, Noah and Razavi, Pedram and Narasimhan, Karthik},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12045},
  eprint = {2406.12045},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12045},
  urldate = {2025-09-26},
  abstract = {Existing benchmarks do not test language agents on their interaction with human users or ability to follow domain-specific rules, both of which are vital for deploying them in real world applications. We propose \${\textbackslash}tau\$-bench, a benchmark emulating dynamic conversations between a user (simulated by language models) and a language agent provided with domain-specific API tools and policy guidelines. We employ an efficient and faithful evaluation process that compares the database state at the end of a conversation with the annotated goal state. We also propose a new metric (pass{\textasciicircum}k) to evaluate the reliability of agent behavior over multiple trials. Our experiments show that even state-of-the-art function calling agents (like gpt-4o) succeed on {$<$}50\% of the tasks, and are quite inconsistent (pass{\textasciicircum}8 {$<$}25\% in retail). Our findings point to the need for methods that can improve the ability of agents to act consistently and follow rules reliably.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/P39GILUM/Yao et al. - 2024 - $τ$-bench A Benchmark for Tool-Agent-User Interaction in Real-World Domains.pdf;/Users/preetams/Zotero/storage/282MWUPH/2406.html}
}

@misc{yaoReActSynergizingReasoning2023a,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2023},
  month = mar,
  number = {arXiv:2210.03629},
  eprint = {2210.03629},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2025-09-26},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: v3 is the ICLR camera ready version with some typos fixed. Project site with code: https://react-lm.github.io},
  file = {/Users/preetams/Zotero/storage/62N8JBFF/Yao et al. - 2023 - ReAct Synergizing Reasoning and Acting in Language Models.pdf;/Users/preetams/Zotero/storage/V3VDFQIF/2210.html}
}

@misc{yuCREPEOpenDomainQuestion2022,
  title = {{{CREPE}}: {{Open-Domain Question Answering}} with {{False Presuppositions}}},
  shorttitle = {{{CREPE}}},
  author = {Yu, Xinyan Velocity and Min, Sewon and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  year = {2022},
  month = nov,
  number = {arXiv:2211.17257},
  eprint = {2211.17257},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.17257},
  urldate = {2025-09-26},
  abstract = {Information seeking users often pose questions with false presuppositions, especially when asking about unfamiliar topics. Most existing question answering (QA) datasets, in contrast, assume all questions have well defined answers. We introduce CREPE, a QA dataset containing a natural distribution of presupposition failures from online information-seeking forums. We find that 25\% of questions contain false presuppositions, and provide annotations for these presuppositions and their corrections. Through extensive baseline experiments, we show that adaptations of existing open-domain QA models can find presuppositions moderately well, but struggle when predicting whether a presupposition is factually correct. This is in large part due to difficulty in retrieving relevant evidence passages from a large text corpus. CREPE provides a benchmark to study question answering in the wild, and our analyses provide avenues for future work in better modeling and further studying the task.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/EVF9T6ZM/Yu et al. - 2022 - CREPE Open-Domain Question Answering with False Presuppositions.pdf;/Users/preetams/Zotero/storage/ZLUAM6G3/2211.html}
}

@misc{yuIfQADatasetOpendomain2023,
  title = {{{IfQA}}: {{A Dataset}} for {{Open-domain Question Answering}} under {{Counterfactual Presuppositions}}},
  shorttitle = {{{IfQA}}},
  author = {Yu, Wenhao and Jiang, Meng and Clark, Peter and Sabharwal, Ashish},
  year = {2023},
  month = may,
  number = {arXiv:2305.14010},
  eprint = {2305.14010},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.14010},
  urldate = {2025-09-26},
  abstract = {Although counterfactual reasoning is a fundamental aspect of intelligence, the lack of large-scale counterfactual open-domain question-answering (QA) benchmarks makes it difficult to evaluate and improve models on this ability. To address this void, we introduce the first such dataset, named IfQA, where each question is based on a counterfactual presupposition via an "if" clause. For example, if Los Angeles was on the east coast of the U.S., what would be the time difference between Los Angeles and Paris? Such questions require models to go beyond retrieving direct factual knowledge from the Web: they must identify the right information to retrieve and reason about an imagined situation that may even go against the facts built into their parameters. The IfQA dataset contains over 3,800 questions that were annotated annotated by crowdworkers on relevant Wikipedia passages. Empirical analysis reveals that the IfQA dataset is highly challenging for existing open-domain QA methods, including supervised retrieve-then-read pipeline methods (EM score 36.2), as well as recent few-shot approaches such as chain-of-thought prompting with GPT-3 (EM score 27.4). The unique challenges posed by the IfQA benchmark will push open-domain QA research on both retrieval and counterfactual reasoning fronts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/6QQSYP8U/Yu et al. - 2023 - IfQA A Dataset for Open-domain Question Answering under Counterfactual Presuppositions.pdf;/Users/preetams/Zotero/storage/II279X56/2305.html}
}

@misc{zengRARERetrievalAwareRobustness2025,
  title = {{{RARE}}: {{Retrieval-Aware Robustness Evaluation}} for {{Retrieval-Augmented Generation Systems}}},
  shorttitle = {{{RARE}}},
  author = {Zeng, Yixiao and Cao, Tianyu and Wang, Danqing and Zhao, Xinran and Qiu, Zimeng and Ziyadi, Morteza and Wu, Tongshuang and Li, Lei},
  year = {2025},
  month = jun,
  number = {arXiv:2506.00789},
  eprint = {2506.00789},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.00789},
  urldate = {2025-09-26},
  abstract = {Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and policy documents and 48,322 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our results show that RAG systems exhibit surprising vulnerability to perturbations, with document robustness consistently being the weakest point regardless of generator size or architecture. RAG systems consistently show lower robustness on multi-hop queries than single-hop queries across all domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/R9NRE47X/Zeng et al. - 2025 - RARE Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems.pdf;/Users/preetams/Zotero/storage/3MQSEMJ4/2506.html}
}

@misc{zhangDARGDynamicEvaluation2024,
  title = {{{DARG}}: {{Dynamic Evaluation}} of {{Large Language Models}} via {{Adaptive Reasoning Graph}}},
  shorttitle = {{{DARG}}},
  author = {Zhang, Zhehao and Chen, Jiaao and Yang, Diyi},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17271},
  eprint = {2406.17271},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.17271},
  urldate = {2025-09-26},
  abstract = {The current paradigm of evaluating Large Language Models (LLMs) through static benchmarks comes with significant limitations, such as vulnerability to data contamination and a lack of adaptability to the evolving capabilities of LLMs. Therefore, evaluation methods that can adapt and generate evaluation data with controlled complexity are urgently needed. In this work, we introduce Dynamic Evaluation of LLMs via Adaptive Reasoning Graph Evolvement (DARG) to dynamically extend current benchmarks with controlled complexity and diversity. Specifically, we first extract the reasoning graphs of data points in current benchmarks and then perturb the reasoning graphs to generate novel testing data. Such newly generated test samples can have different levels of complexity while maintaining linguistic diversity similar to the original benchmarks. We further use a code-augmented LLM to ensure the label correctness of newly generated data. We apply our DARG framework to diverse reasoning tasks in four domains with 15 state-of-the-art LLMs. Experimental results show that almost all LLMs experience a performance decrease with increased complexity and certain LLMs exhibit significant drops. Additionally, we find that LLMs exhibit more biases when being evaluated via the data generated by DARG with higher complexity levels. These observations provide useful insights into how to dynamically and adaptively evaluate LLMs. The code is available at https://github.com/SALT-NLP/DARG.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/preetams/Zotero/storage/NZJYUEWP/Zhang et al. - 2024 - DARG Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph.pdf;/Users/preetams/Zotero/storage/8SR2MTLD/2406.html}
}

@misc{zhangSituatedQAIncorporatingExtraLinguistic2021,
  title = {{{SituatedQA}}: {{Incorporating Extra-Linguistic Contexts}} into {{QA}}},
  shorttitle = {{{SituatedQA}}},
  author = {Zhang, Michael J. Q. and Choi, Eunsol},
  year = {2021},
  month = sep,
  number = {arXiv:2109.06157},
  eprint = {2109.06157},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.06157},
  urldate = {2025-09-26},
  abstract = {Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g., roughly 16.5\% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https://situatedqa.github.io/ .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: Accepted at EMNLP 2021},
  file = {/Users/preetams/Zotero/storage/9UYIHJJG/Zhang and Choi - 2021 - SituatedQA Incorporating Extra-Linguistic Contexts into QA.pdf;/Users/preetams/Zotero/storage/GT5NZ7CI/2109.html}
}

@article{zhangSurveyMemoryMechanism2025,
  title = {A {{Survey}} on the {{Memory Mechanism}} of {{Large Language Model-based Agents}}},
  author = {Zhang, Zeyu and Dai, Quanyu and Bo, Xiaohe and Ma, Chen and Li, Rui and Chen, Xu and Zhu, Jieming and Dong, Zhenhua and Wen, Ji-Rong},
  year = {2025},
  month = sep,
  journal = {ACM Trans. Inf. Syst.},
  volume = {43},
  number = {6},
  pages = {155:1--155:47},
  issn = {1046-8188},
  doi = {10.1145/3748302},
  urldate = {2025-09-26},
  abstract = {Large language model (LLM)-based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this article, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ``what is'' and ``why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at .},
  file = {/Users/preetams/Zotero/storage/8A5RI2KM/Zhang et al. - 2025 - A Survey on the Memory Mechanism of Large Language Model-based Agents.pdf}
}

@misc{zhouDontMakeYour2023,
  title = {Don't {{Make Your LLM}} an {{Evaluation Benchmark Cheater}}},
  author = {Zhou, Kun and Zhu, Yutao and Chen, Zhipeng and Chen, Wentong and Zhao, Wayne Xin and Chen, Xu and Lin, Yankai and Wen, Ji-Rong and Han, Jiawei},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01964},
  eprint = {2311.01964},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.01964},
  urldate = {2025-09-26},
  abstract = {Large language models{\textasciitilde}(LLMs) have greatly advanced the frontiers of artificial intelligence, attaining remarkable improvement in model capacity. To assess the model performance, a typical approach is to construct evaluation benchmarks for measuring the ability level of LLMs in different aspects. Despite that a number of high-quality benchmarks have been released, the concerns about the appropriate use of these benchmarks and the fair comparison of different models are increasingly growing. Considering these concerns, in this paper, we discuss the potential risk and impact of inappropriately using evaluation benchmarks and misleadingly interpreting the evaluation results. Specially, we focus on a special issue that would lead to inappropriate evaluation, {\textbackslash}ie {\textbackslash}emph\{benchmark leakage\}, referring that the data related to evaluation sets is occasionally used for model training. This phenomenon now becomes more common since pre-training data is often prepared ahead of model test. We conduct extensive experiments to study the effect of benchmark leverage, and find that it can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance. To improve the use of existing evaluation benchmarks, we finally present several guidelines for both LLM developers and benchmark maintainers. We hope this work can draw attention to appropriate training and evaluation of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  note = {Comment: 11 pages},
  file = {/Users/preetams/Zotero/storage/FW2XGQFY/Zhou et al. - 2023 - Don't Make Your LLM an Evaluation Benchmark Cheater.pdf;/Users/preetams/Zotero/storage/R43BIGFI/2311.html}
}

@misc{zhuDynamicEvaluationLarge2024,
  title = {Dynamic {{Evaluation}} of {{Large Language Models}} by {{Meta Probing Agents}}},
  author = {Zhu, Kaijie and Wang, Jindong and Zhao, Qinlin and Xu, Ruochen and Xie, Xing},
  year = {2024},
  month = jun,
  number = {arXiv:2402.14865},
  eprint = {2402.14865},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.14865},
  urldate = {2025-09-26},
  abstract = {Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal{\textasciitilde}{\textbackslash}citep\{zhu2023dyval\}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs. Code is available at: https://github.com/microsoft/promptbench.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: International Conference on Machine Learning (ICML) 2024},
  file = {/Users/preetams/Zotero/storage/SGC3JAED/Zhu et al. - 2024 - Dynamic Evaluation of Large Language Models by Meta Probing Agents.pdf;/Users/preetams/Zotero/storage/QN3IKAYQ/2402.html}
}

@misc{zhuDyValDynamicEvaluation2024b,
  title = {{{DyVal}}: {{Dynamic Evaluation}} of {{Large Language Models}} for {{Reasoning Tasks}}},
  shorttitle = {{{DyVal}}},
  author = {Zhu, Kaijie and Chen, Jiaao and Wang, Jindong and Gong, Neil Zhenqiang and Yang, Diyi and Xie, Xing},
  year = {2024},
  month = mar,
  number = {arXiv:2309.17167},
  eprint = {2309.17167},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.17167},
  urldate = {2025-09-26},
  abstract = {Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: ICLR 2024 spotlight; 38 pages; code is at aka.ms/dyval},
  file = {/Users/preetams/Zotero/storage/TY8MXUA5/Zhu et al. - 2024 - DyVal Dynamic Evaluation of Large Language Models for Reasoning Tasks.pdf;/Users/preetams/Zotero/storage/H4LZFL5P/2309.html}
}

@article{zhuLargeLanguageModels2025,
  title = {Large {{Language Models}} for {{Information Retrieval}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Information Retrieval}}},
  author = {Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan and Liu, Wenhan and Deng, Chenlong and Chen, Haonan and Liu, Zheng and Dou, Zhicheng and Wen, Ji-Rong},
  year = {2025},
  month = sep,
  journal = {ACM Transactions on Information Systems},
  eprint = {2308.07107},
  primaryclass = {cs},
  pages = {3748304},
  issn = {1046-8188, 1558-2868},
  doi = {10.1145/3748304},
  urldate = {2025-09-26},
  abstract = {As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  note = {Comment: Updated to version 4; Accepted by ACM TOIS},
  file = {/Users/preetams/Zotero/storage/ZIU743KZ/Zhu et al. - 2025 - Large Language Models for Information Retrieval A Survey.pdf;/Users/preetams/Zotero/storage/ELSSZKLQ/2308.html}
}
