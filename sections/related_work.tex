\section{Related Work}

\subsection{Preliminaries}

\subsubsection{Benchmark Leakage}

Benchmark leakage refers to the contamination that occurs when a model has prior exposure to a benchmark's test items during pretraining, leading to inflated evaluation scores and undermining the benchmark's validity \cite{zhouDontMakeYour2023,zhuDyValDynamicEvaluation2024b}. Since most LLMs are pretrained indiscriminately on publicly available web content, there is a high likelihood that they might have encountered benchmark datasets hosted on platforms like HuggingFace, GitHub, Kaggle, and similar sites. If the base LLM used in a WebQA agent has already seen the test examples during pretraining, the evaluation effectively becomes a case of testing on the training set — severely compromising the credibility of the reported results.

\subsubsection{Dynamic Benchmarking}

Dynamic benchmarking is an emerging evaluation paradigm that generates or adapts test samples at evaluation time, so models are scored on fresh, unseen data and testset contamination is avoided \cite{zhuDyValDynamicEvaluation2024b,kielaDynabenchRethinkingBenchmarking2021a,maDynaboardEvaluationAsAServicePlatform2021,zhuDynamicEvaluationLarge2024,zhangDARGDynamicEvaluation2024,rawlesAndroidWorldDynamicBenchmarking2025}. That means the agent does not see the same test sample twice, effectively addressing issues such as LLM memorization.

\subsubsection{Evolving Information}

In this work, we refer to information that can change over time, such as the stock price of Tesla, as evolving information. Arguably, this type of content could be labeled as dynamic content or temporal content, but that might cause confusion, since we are already using the term ``dynamic'' to describe an evaluation paradigm (dynamic benchmarking), and ``temporal'' might lead readers to associate it with time-series data. To avoid such confusion, we adopt the term evolving information to specifically denote factual content that may shift over time without implying a particular format or evaluation framework.

\subsection{Related Work}

\subsubsection{Open-Domain QA and Retrieval-Augmented Agents}

Open-domain question answering traditionally involves retrieving relevant documents and extracting or generating an answer. For instance, consider DrQA \cite{chenReadingWikipediaAnswer2017a}, which combined a TF-IDF document retriever with a neural reader to answer questions using Wikipedia. Since then, large-scale datasets like Natural Questions \cite{kwiatkowskiNaturalQuestionsBenchmark2019a}, TriviaQA \cite{joshiTriviaQALargeScale2017} and HotPotQA \cite{yangHotpotQADatasetDiverse2018a} have driven progress in retrieval-based QA, with models achieving impressive in-domain accuracy. The introduction of transformer-based retrievers and readers led to substantial gains – for example, dense passage retrieval and a fusion-in-decoder reader (FiD) improved open QA by aggregating information from multiple passages \cite{izacardLeveragingPassageRetrieval2021}. More recently, the trend has shifted to retrieval-augmented generation (RAG) systems \cite{yaoReActSynergizingReasoning2023a}, which integrate retrieval into the generative process. In a RAG model, an LLM conditions on retrieved text chunks when constructing its answer, thereby injecting fresh knowledge and reducing factual errors. RAG-based approaches have become a standard for knowledge-intensive tasks, demonstrating superior factual accuracy and generality \cite{yaoReActSynergizingReasoning2023a}.

With improvements in LLM reasoning capabilities, research has shifted towards agentic frameworks where a model iteratively interacts with tools, such as search engines and knowledge graphs, to answer queries. For example, WebGPT \cite{nakanoWebGPTBrowserassistedQuestionanswering2022a} augments an LLM with the ability to issue web search queries and navigate webpages, guided by human feedback to produce high-quality answers with citations. Other works like ReAct \cite{yaoReActSynergizingReasoning2023a} combines reasoning steps with tool use, allowing the model to plan multi-step solutions (e.g. search for a fact, then use a calculator) in a single unified prompting framework. These ``LLM-as-agent'' approaches are promising because they mimic how humans gather and verify information, and they can handle more complex queries that require multi-hop reasoning or cross-referencing sources \cite{zhuLargeLanguageModels2025}. 

\subsubsection{Robustness of QA Systems}

Robustness of QA systems has garnered a lot of interest as researchers realized that high IID (in-distribution) test scores do not guarantee real-world reliability \cite{thakurBEIRHeterogenousBenchmark2021}. Prior research has examined multiple facets of robustness. One line of work looks at adversarial robustness: for example, adding misleading but irrelevant sentences to a passage can confuse models that lack true understanding \cite{jiaAdversarialExamplesEvaluating2017}. This revealed that many QA models rely on shallow cues and can be tricked by simple perturbations. In open-domain QA, models must also cope with naturally occurring distractions or errors in retrieved text. The HotpotQA dataset \cite{yangHotpotQADatasetDiverse2018a}, which requires multi-hop reasoning across multiple Wikipedia articles and includes some irrelevant paragraphs, was an early attempt to test a model's ability to stay focused on relevant facts. Dense retrievers and readers have been shown to drop in accuracy when deployed on different source distributions or domains – for instance, a model trained on Wikipedia may struggle on biomedical articles. The BEIR benchmark \cite{thakurBEIRHeterogenousBenchmark2021} quantified this by evaluating retrieval models on 18 heterogeneous IR tasks: no single model performed uniformly well across all domains, highlighting generalization gaps.

Another crucial aspect is temporal robustness. QA models quickly become outdated as world knowledge changes. To address this, temporal benchmarks have been proposed to evaluate systems on questions about current events or facts that change over time. RealTime QA \cite{kasaiRealTimeQAWhats2024}, for example, continually releases new questions (on a weekly basis) about recent news and evaluates systems' ability to answer using up-to-date information. Similarly, FreshQA \cite{vuFreshLLMsRefreshingLarge2023} is a dataset of time-sensitive questions where answers need to be periodically refreshed to remain correct. These benchmarks show that without retrieval augmentation, LLMs fail completely on questions beyond their training cutoff, and even with retrieval, systems must be robust to latency (documents may not yet reflect the latest answers) and potential contradictions between old and new information.

Closely related is the challenge of conflicting evidence. In realistic web search, not all sources agree – some may have incorrect or outdated information. \cite{liuOpenDomainQuestion2025} introduced the QACC dataset to study how QA systems handle conflicting contexts: they found that as many as 25\% of straightforward factoid questions yield conflicting answers on the web, and current LLM-based QA systems often stumble in these cases, either averaging contradictory statements or choosing incorrectly. Another recent work, RARE (Retrieval-Aware Robustness Evaluation) \cite{zengRARERetrievalAwareRobustness2025}, proposes a unified framework to stress-test RAG models by introducing controlled perturbations at the query and document level. RARE generates variants of questions (e.g. paraphrases or altered facts) and of documents (inserting noise or updating facts) to evaluate if a system remains correct or can recover when its inputs change. Using a time-sensitive fixed set of documents, RARE showed that state-of-the-art RAG systems are brittle: for instance, they are most vulnerable to document perturbations (altered or conflicting content) and degrade significantly on multi-hop questions compared to single-hop ones \cite{zengRARERetrievalAwareRobustness2025}. These findings reinforce the importance of developing QA agents that maintain high fidelity in the face of distribution shifts — whether those are shifts in language (paraphrasing), content (new or conflicting facts), or context over time.

\subsubsection{Dynamic Benchmarking}

Most of the benchmarks currently used for evaluating agentic QA systems remain predominantly static, consisting of fixed samples that are publicly accessible, such as HotPotQA \cite{yangHotpotQADatasetDiverse2018a}. Studies show significant evidence that many widely used static datasets have already been contaminated, rendering them unreliable \cite{zhouDontMakeYour2023,zhuDyValDynamicEvaluation2024b}. In \cite{zhouDontMakeYour2023}, the authors refer to this issue as benchmark leakage -- a growing concern as large foundation models are trained on web-scale datasets encompassing vast portions of publicly available internet data. Studies indicate that even simple paraphrasing can degrade performance, emphasizing the brittle nature of these systems and their reliance on memorization \cite{zhuDyValDynamicEvaluation2024b}.

To overcome these limitations, dynamic benchmarks have emerged as a promising alternative \cite{zhuDyValDynamicEvaluation2024b,kielaDynabenchRethinkingBenchmarking2021a,maDynaboardEvaluationAsAServicePlatform2021,zhuDynamicEvaluationLarge2024,zhangDARGDynamicEvaluation2024}. Unlike their static counterparts, dynamic benchmarks are designed to resist memorization and provide a more accurate assessment of adaptive, context-aware performance in evolving scenarios \cite{zhuDyValDynamicEvaluation2024b,zhuDynamicEvaluationLarge2024,rawlesAndroidWorldDynamicBenchmarking2025,yao$t$benchBenchmarkToolAgentUser2024}. This shift is particularly important for ensuring that reported evaluations reflect genuine advancements in capabilities rather than superficial performance gains.

Early attempts at dynamic benchmarking rely on crowdsourcing for data collection \cite{kielaDynabenchRethinkingBenchmarking2021a,maDynaboardEvaluationAsAServicePlatform2021}, making them costly and difficult to scale. More recent approaches leverage graph-based methods to generate test samples \cite{zhuDyValDynamicEvaluation2024b,zhuDynamicEvaluationLarge2024,zhangDARGDynamicEvaluation2024}, offering advantages such as controllable complexity and adaptability to evolving requirements — an essential feature given the rapid advancements in foundation models. While these approaches were proven to be effective in reasoning tasks \cite{zhuDyValDynamicEvaluation2024b,zhuDynamicEvaluationLarge2024,zhangDARGDynamicEvaluation2024}, dynamic benchmarking for open-domain QA systems remains largely unexplored. A recent study, Dynamic-KGQA \cite{dammuDynamicKGQAScalableFramework2025a}, represents an early attempt at dynamic evaluation of QA systems but relies on knowledge graphs (KGs), which limits the benchmark to information stored in KGs. While KGs are updated periodically, the changes are not instantaneous, and supporting fast-evolving QA pairs becomes infeasible.
