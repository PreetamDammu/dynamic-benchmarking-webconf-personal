
# [One Pager] Dynamic Evaluation of WebQA Agents

## Overview

LLM agents with web access are increasingly becoming the default option for user-facing question-answering systems, as evidenced by the rising adoption of tools like SearchGPT and Perplexity. However, most benchmarks and datasets used to evaluate these systems were originally designed for a previous generation of QA systems and are static in nature, making them susceptible to data leakage. ***Benchmark leakage*** refers to the contamination that occurs when a model has prior exposure to test items during pretraining, leading to inflated evaluation scores and undermining the benchmark'92s credibility. Since LLMs are often pretrained indiscriminately on publicly available web content, there's a high likelihood that test examples have already been seen during training. This turns evaluation into a case of "testing on the training set," which severely compromises the validity of the reported results. ***Dynamic benchmarking*** offers a promising alternative by generating test samples at evaluation time. This ensures that models are evaluated on fresh, unseen data and reduces the risk of test-set contamination.

**Objective:** Develop a dynamic QA benchmark suitable for question-answering agents equipped with web search capabilities, with built-in resistance to data leakage and memorization.

**Preliminaries and Related Work:** [[Related Work] Dynamic Evaluation of WebQA Agents](https://quip-amazon.com/GQfaAI002kpg)

**Why dynamic evaluations?**

* **Pretraining Contamination**: Any static dataset or benchmark made publicly available is likely to have been seen during LLM pretraining. While some benchmarks like [GAIA](https://arxiv.org/abs/2311.12983) release only questions and withhold answers to mitigate this, such an approach introduces transparency concerns and limits access for researchers. It also requires benchmark providers to run evaluations on behalf of all users, which may be infeasible at scale due to compute constraints.
* **The Web is Dynamic**: A second motivation stems from the evolving nature of the web itself. Many user queries are time-sensitive: for example, '93Does Tom Cruise have a movie coming out this month?'94, and static datasets like HotPotQA, which contain immutable questions and answers, are not suited for such scenarios. Benchmarks like [FreshQA](https://github.com/freshllms/freshqa) and [RealTimeQA](https://arxiv.org/abs/2207.13332) address this by updating content weekly, but they still fall short when it comes to rapidly changing topics like stock prices, traffic, or weather conditions.
* **Broader Coverage of Real-world Queries**: A truly dynamic benchmark can support these fast-evolving, time-sensitive queries '97 questions that are highly relevant in end-user applications. This becomes possible as test samples are constructed at run time, grounded in curated and verifiable data. 


Proposed Method Draft: [[Method] Dynamic Evaluation of WebQA Agents](https://quip-amazon.com/b2HgAfPdfFlm)



