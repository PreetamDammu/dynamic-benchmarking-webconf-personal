

# [Experiments & Results] Dynamic Evaluation of WebQA Agents

Graph representations are ideal for anchoring dynamic test sample generation, as they help in controlling complexity and ensuring consistency across different runs. Previous works show that they are effective for reasoning and KGQA [[1](https://arxiv.org/abs/2309.17167), [2](https://arxiv.org/abs/2503.05049)]. As there are various artifacts that need to be captured when representing the task executions of WebQA agents as graphs, we use the following graph schema.

### Nodes

|Vertex Class	|Label	|Payload	|
|---	|---	|---	|
|Query	|vq	|raw text q	|
|Web Document	|vd	|HTML page	|
|Text Snippet	|vs	|agent observations, thoughts	|
|Answer	|vy*t	|gold answer text	|

### Edges

|Edge Label	|Represents	|
|---	|---	|
|
retrieve	|
document fetched via search or follow-up visit	|
|
evidence	|
snippet extracted; snippet supports answer	|
|
comparison	|
numerical/logical comparison between snippets	|
|
hypothesis	|
agent'92s conjecture as an intermediate snippet	|
|
conclude	|
final reasoning step deriving the answer	|
|
follows	|
snippet generated immediately after another	|

#### Generating Execution Logs

For the 500 questions from FreshQA dataset ([paper](https://arxiv.org/abs/2310.03214), [dataset](https://github.com/freshllms/freshqa)), we use the Perplexica framework ([link](https://github.com/ItzCrazyKns/Perplexica)) to generate raw data. Perplexica operates similarly to the popular agentic search tool [Perplexity](https://www.perplexity.ai/), and uses [SearxNG](https://github.com/searxng/searxng) search engine to augment an LLM of our choice (we use deepseek.r1-v1:0 in our experiments).

#### An example log

`**'question':** 'When did Meta move its headquarters to Austin?'`
`**'answer':** "It hasn't moved to Austin" `
`**'thinking':** Based on the provided context, Meta has NOT moved its headquarters to Austin. While there are discussions about Meta potentially reincorporating in Texas [8][10], a Meta spokesperson explicitly stated that "it does not plan on shifting its corporate headquarters out of Menlo Park, California" [8].fact, Meta has been reducing its presence in Austin by:- Leaving its lease at The Domain [4][9]- Subleasing 120,000 square feet of downtown space [6]'`


```
'sources': [{'id': 1,
   'title': 'Meta in Texas: Downtown Austin renovations, solar power purchase near ...',
   'url': 'https://www.kvue.com/article/money/economy/boomtown-2040/meta-downtown-austin-renovations-solar-bastrop/269-90784b92-0226-4dd6-97ae-9475da00d591',
   'snippet': 'Mar 19, 2025 ... Meta appears to be renovating its Downtown Austin office space. The company also plans to buy all the power from a planned solar farm in...'},
  {'id': 2,
   'title': 'META to move teams to Texas - cbsaustin.com',
   'url': 'https://cbsaustin.com/news/local/meta-to-move-teams-to-texas',
   'snippet': 'AUSTIN, Texas '97 CEO of META, Mark Zuckerberg, announced content oversight teams will be relocating to Texas. The announcement came in a video he posted to Facebook Tuesday morning. "We're moving our trust and safety, and content moderation teams out of California, and our US-based content review ...'},
  {'id': 3,
   'title': "Facebook Moving To Texas? Parent Company 'Meta' Signs Massive Lease In ...",
   'url': 'https://www.dailywire.com/news/facebook-moving-to-texas-parent-company-meta-signs-massive-lease-in-downtown-austin',
   'snippet': 'Facebook's parent company Meta has become the latest California corporation to at least partly move to Texas as it has signed a massive lease called "the largest ever in downtown Austin." "The lease is the largest ever in Downtown Austin and larger than the entire Frost Bank Tower in terms of square feet," KVUE reported.'},
  {'id': 4,
   'title': 'Meta Is Leaving Its Austin Office, Guess What Big Tech Company Is Moving In',
   'url': 'https://finance.yahoo.com/news/meta-leaving-austin-office-guess-211425144.html',
   'snippet': "The office real estate picture in Austin, TX is constantly changing but one company's loss may be another company's gain. Meta (NASDAQ:META) is leaving behind its lease in The Domain, a mixed ..."},
  {'id': 5,
   'title': 'Meta plans $2.9 million renovation in another Texas office space',
   'url': 'https://www.chron.com/culture/article/meta-austin-office-20221773.php',
   'snippet': 'Mar 14, 2025 ... Meta, the parent company of Facebook, Instagram and WhatsApp, is working on a tenant fit-up of a downtown Austin office at 300 West Sixth St.'},
  {'id': 6,
   'title': 'Meta out, IBM in at the Domain | Urbanize Austin',
   'url': 'https://austin.urbanize.city/post/austin-meta-ibm-domain-sublease-cousins',
   'snippet': 'Sep 6, 2024 ... Facebook parent company Meta laid off more than 10,000 employees and announced a 120,000-square-foot sublease in a downtown Austin building in...'},
  {'id': 7,
   'title': 'Offices - Meta',
   'url': 'https://www.meta.com/media-gallery/offices-around-the-world/',
   'snippet': 'Meta headquarters, campuses and offices around the world. Media available for download.'},
  {'id': 8,
   'title': 'Meta in talks to reincorporate in Texas or another state, exit ...',
   'url': 'https://www.reuters.com/technology/meta-talks-reincorporate-texas-or-another-state-exit-delaware-wsj-reports-2025-01-31/',
   'snippet': 'Jan 31, 2025 ... A Meta spokesperson said that it does not plan on shifting its corporate headquarters out of Menlo Park, California, but declined to comment...'},
  {'id': 9,
   'title': "IBM takes over Meta's lease at The Domain in Austin - MySA",
   'url': 'https://www.mysanantonio.com/news/austin/article/meta-ibm-austin-19752226.php',
   'snippet': "Sep 9, 2024 '97 IBM will be taking over Meta's former lease on a 320,000-square foot building at The Domain in North Austin. While some tech companies are ..."},
  {'id': 10,
   'title': "Is Facebook parent-company Meta moving to Texas? Here's what we know.",
   'url': 'https://www.statesman.com/story/business/technology/2025/01/31/facebook-instagram-meta-moving-to-texas/78093842007/',
   'snippet': 'Facebook and Instagram parent company, Meta Platforms, is reportedly in talks to reincorporate in Texas.'},
  {'id': 11,
   'title': "Construction of Meta's data center campus north of Austin to ...",
   'url': 'https://www.bizjournals.com/austin/news/2025/03/26/meta-temple-data-center-construction-update-austin.html',
   'snippet': "Mar 26, 2025 ... The project is part of Meta's growing operations in the Lone Star State. Meta has indicated that it's considering moving its headquarters to..."},
  {'id': 12,
   'title': 'Meta Has Bailed Out of Nearly 1 Million-SF of Austin Office Space',
   'url': 'https://www.connectcre.com/stories/meta-has-bailed-out-of-nearly-1-million-sf-of-office-space-in-austin/',
   'snippet': 'Bailing out on large leases has become all too common of late. Remote work and high interest rates have played havoc within the office sector. Facebook parent company Meta Platforms is shedding another big chunk of Austin office space. It wants out of a 320,000 office building in The Domain in North Austin. The Austin Business Journal ...'},
  {'id': 13,
   'title': 'Meta eyes $2.9M project at downtown office building - Austin ...',
   'url': 'https://www.bizjournals.com/austin/news/2025/03/12/meta-downtown-austin-office.html',
   'snippet': 'Mar 12, 2025 '97 Meta said in June 2023 that it was trying to find a subtenant for the nearly 120,000 square feet it leases in the building because it planned to ...'},
  {'id': 14,
   'title': 'The Facebook Company Is Now Meta | Meta',
   'url': 'https://about.fb.com/news/2021/10/facebook-company-is-now-meta/',
   'snippet': 'Oct 28, 2021 'b7 What is Meta? Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way '85'},
  {'id': 15,
   'title': 'Introducing Oakley Meta Glasses, a New Category of Performance '85',
   'url': 'https://about.fb.com/news/2025/06/introducing-oakley-meta-glasses-a-new-category-of-performance-ai-glasses/',
   'snippet': 'Jun 20, 2025 'b7 Meta and Oakley are collaborating to deliver a brand-new category of Performance AI glasses. Oakley Meta glasses are a new product line that will combine Oakley'92s signature '85'}]}
```

#### Constructing Graphs

Since the agent execution logs are unstructured and presented as agent thoughts, constructing a graph requires parsing the raw text and creating a structured set of steps. An LLM can be used to implement this step, with the following prompt:


```
You are a JSON formatter.

INPUT = (<context> + <question> + <answer>) + TRANSCRIPT

SPLIT RULES
Split TRANSCRIPT into bullets / blank-line paragraphs.
If a bullet has several citations '93[n]'94, create a separate THOUGHT
   for each citation, duplicating the claim text but keeping one '93[n]'94.

STEP-TYPE
conclude  '96 last THOUGHT before the answer OR begins with
              '93overall'94, '93in summary'94, '93therefore'94, '93thus'94, '93hence'94 '85
evidence  '96 contains 1 '93[n]'94 AND no reasoning words (because, implies '85)
reason    '96 all others.
You are a JSON formatter.

INPUT = (<context> + <question> + <answer>) + TRANSCRIPT

Split TRANSCRIPT into bullets / blank-line paragraphs.
If one bullet has several citations '93[n]'94, duplicate it so each
     new bullet carries exactly one '93[n]'94.

For each resulting bullet produce an object
   { "text": <bullet>, "type": <STEP-TYPE> } using:

   STEP-TYPE
     conclude   '96 last bullet before the answer OR starts with
                  '93overall / in summary / therefore / thus / hence'94.
     evidence   '96 contains 1 '93[n]'94 AND no reasoning words
                  (because, implies, therefore, might, etc.).
     retrieve   '96 issues a search or URL visit .
     comparison '96 compares two facts (compared, versus, higher than '85).
     hypothesis '96 speculative statement (maybe, might, could, assume).
     reason     '96 anything else (explanatory but not covered above).

Output a JSON array called **thoughts** '97 no extra keys, no commentary.
OUTPUT FORMAT:

{
  "thoughts":[
    {"id":1,"text":"...","step_type":"evidence|reason|conclude",
     "cites":[{"id":n,"stance":"supports|contradicts|neutral"}]},
    '85
  ],
  "answer":{"text":"...", "cites":[{"id":n,"stance":"..."}]}
}
```


Once the JSON with sequential thoughts and step types is obtained, we can construct graphs such as the ones shown below:
[Image: image.png]

As can be observed, there are several nodes that are not visited. Exploring other parts of the graphs and visiting nodes that were not utilized in the answering the original can help in generating related QA pairs. However, after exploring this approach, a few drawbacks were identified:

* LLM agents make mistakes (expected behavior as performance on FreshQA dataset <80%), and relying on them to solve multi-hop questions may not be the best approach.
* It is non-trivial to localize errors when things go wrong.
* Little control on what claims within a document are utilized for QA generation.


A better approach would be to extract claims within certain documents of interest, and then construct the questions bottom up. This provides higher control over the selection of documents, claims, reasoning operations required to answer questions, number of hops and complexity of the question in general.

#### Example

`**FreshQA Question:** What is the name of Leonardo DiCaprio's third child? `
**`FreshQA Answer:`**` Leonardo DiCaprio does not have any children.`

**`Perplexica (w/ DeepSeek-R1) Answer: `**`He has no children. `
`**Sources:** Fetched 15 sources. `
`**Thinking:** Based on the context, Leonardo DiCaprio does not have any children. Multiple sources confirm this, including [1] which states that "kids aren't a priority in his life" and [12] which explicitly states "Leonardo DiCaprio may not have any children."`

Now, to generate related questions (about Leonardo DiCaprio'92s personal life), we can randomly pick a few sources from the retrieved 15 HTML docs. If we were to select 3 documents out of these 15, there could be 15c3 or 455 combinations. While we are using just using top 15 search results for now, this number can be much larger if higher diversity is required.

Next, atomic claims from the long-form text from each document are extracted, similar to FactScore ([link](https://arxiv.org/pdf/2305.14251)).

For example, let'92s assume documents `[7, 15, 8]` were selected. Extracted claims would look like:

Note: The following values are for doc [1,4,8] since I extracted these values after rerunning the cell, but the response for any docs would look similar. The prompt can be found in  [Prompts Section](https://quip-amazon.com/aMnQADJbkxmH#temp:C:HDFa0901ef2572849d6bf672f8b2).

```
{1: {'claim1': 'Leonardo DiCaprio has not fathered any children during his time in Hollywood.',
  'supporting_text_span1': 'The rumors have all been false, as Leo has neither been married nor fathered a child since his time in Hollywood.',
  'claim2': 'Leonardo DiCaprio expressed ambiguous thoughts about having children in a 2016 Rolling Stone interview.',
  'supporting_text_span2': '"Do you mean do I want to bring children into a world like this?" he told Rolling Stone in 2016. "If it happens, it happens. I'd prefer not to get into specifics about it, just because then it becomes something that is misquoted. But, yeah. I don't know... To articulate how I feel about it is just gonna be misunderstood,"',
  'claim3': 'Leonardo DiCaprio dated model and actress Camila Morrone from 2017 until their breakup in August 2022.',
  'supporting_text_span3': "Leo's most recent girlfriend, 22-year-old model and actress Camila Morrone, was rumored to be pregnant a few times over the course of their relationship. The pair started dating in 2017, when Camila was only 20 [...] In August 2022, it was reported that he and Camila broke up",
  'claim4': 'Leonardo DiCaprio has been an advocate for environmental causes including clean energy initiatives and climate change policy.',
  'supporting_text_span4': "The actor has advocated for cleaner energy initiatives and immediate policy to reverse the effects of climate change for years. He's continuously used his platform as an actor to promote his ideas."},
 4: {'claim1': 'Leonardo DiCaprio was born on November 11, 1974, in Los Angeles, California.',
  'supporting_text_span1': 'Born on November 11, 1974, in Los Angeles, California',
  'claim2': 'Leonardo DiCaprio was named after the famous painter Leonardo da Vinci.',
  'supporting_text_span2': "DiCaprio was named after the famous painter Leonardo da Vinci, as his mother felt him kick while looking at one of da Vinci's paintings",
  'claim3': 'Leonardo DiCaprio started his acting career at age five in commercials and educational films.',
  'supporting_text_span3': 'DiCaprio started his acting career at the age of five, appearing in commercials and educational films',
  'claim4': 'Leonardo DiCaprio established Appian Way Productions in 2004.',
  'supporting_text_span4': 'In 2004, DiCaprio established his own production company, Appian Way Productions',
  'claim5': "Leonardo DiCaprio won the Academy Award for Best Actor in 2016 for his role in 'The Revenant'.",
  'supporting_text_span5': "DiCaprio finally won the Academy Award for Best Actor in 2016 for his role in 'The Revenant'",
  'claim6': "Leonardo DiCaprio received his first Academy Award nomination for Best Supporting Actor in 1994 for 'What's Eating Gilbert Grape'.",
  'supporting_text_span6': "For his role in 'What's Eating Gilbert Grape,' DiCaprio received his first Academy Award nomination for Best Supporting Actor in 1994",
  'claim7': "Leonardo DiCaprio's first major television role was as Garry Buckman in the sitcom 'Parenthood' in 1990.",
  'supporting_text_span7': "His first major role on television was in the sitcom 'Parenthood' in 1990, where he played Garry Buckman"},
 11: {'claim1': 'Leonardo DiCaprio was born on November 11, 1974, in Los Angeles, California.',
  'supporting_text_span1': 'Leonardo DiCaprio was born on November 11,1974, in Los Angeles, California.',
  'claim2': 'Leonardo DiCaprio is the only child of Irmelin Indenbirken, a legal secretary, and George DiCaprio, an underground comix artist and distributor.',
  'supporting_text_span2': 'Leonardo DiCaprio is the only child of Irmelin Indenbirken, a legal secretary, and George DiCaprio, an underground comix artist and distributor.',
  'claim3': 'Leonardo DiCaprio founded the production company Appian Way Productions in 2004.',
  'supporting_text_span3': 'In 2004, Leonardo DiCaprio founded the production company Appian Way Productions, a namesake of the Italian road.',
  'claim4': 'Leonardo DiCaprio was paid $30 million for his role in Killers of the Flower Moon.',
  'supporting_text_span4': "Leonardo DiCaprio next starred in Scorsese's crime drama Killers of the Flower Moon based on the book of the same name by David Grann , for which he was paid $30 million.",
  'claim5': 'Leonardo DiCaprio was made a commander of the Ordre des Arts et des Lettres by the French Minister of Culture in 2005.',
  'supporting_text_span5': 'In 2005, Leonardo DiCaprio was made a commander of the Ordre des Arts et des Lettres by the French Minister of Culture for his contributions to the arts.'}}
```

As can be seen above, verbatim excerpts from the documents are extracted for each claim. The supporting text spans can be verified by performing simple string match to ensure there are no hallucinations. These text spans are useful later in the LLM-as-a-Judge phase when all generated datapoints are passed through a verification filter.

The claims are then arranged as separate buckets (as shown below) and fed to the LLM generator using prompts presented in [Prompts Section](https://quip-amazon.com/aMnQADJbkxmH#temp:C:HDFa0901ef2572849d6bf672f8b2).

```


{
    "doc_id": 7,
    "claims": {
      "doc7_claim1": "Leonardo DiCaprio's childhood photo with his parents has gone viral on social media with over 13 million views.",
      "doc7_claim2": "Leonardo DiCaprio won his first Best Actor Oscar for his role in The Revenant.",
      "doc7_claim3": "Leonardo DiCaprio's father worked as an underground comic book distributor and his mother was a legal secretary.",
      "doc7_claim4": "Leonardo DiCaprio's parents worked for his production company in 1995.",
      "doc7_claim5": "Leonardo DiCaprio received the Actors Inspiration Award at the SAG Foundation 30th-anniversary celebration in November 2015."
    }
  },
  {
    "doc_id": 15,
    "claims": {
      "doc15_claim1": "Leonardo DiCaprio bought an island in Belize and built the world's first eco-restorative resort that restores the island's over-fished waters, coastline, and forest.",
      "doc15_claim2": "Leonardo DiCaprio and Kate Winslet helped pay nursing home fees for the last Titanic survivor to prevent her from selling Titanic mementos.",
      "doc15_claim3": "Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.",
      "doc15_claim4": "Leonardo DiCaprio blocked a film he appeared in from being released in the United States and Canada.",
      "doc15_claim5": "Leonardo DiCaprio turned down the role of Anakin Skywalker in the Star Wars prequels.",
      "doc15_claim6": "Leonardo DiCaprio speaks German fluently due to spending part of his childhood in Germany with his grandparents.",
      "doc15_claim7": "The Taliban banned the Leonardo DiCaprio haircut from Titanic among Afghan youths, with barbers facing jail time for giving it.",
      "doc15_claim8": "Leonardo DiCaprio has never used drugs and required training from drug experts to portray being high.",
      "doc15_claim9": "Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street."
    }
  },
  {
    "doc_id": 8,
    "claims": {
      "doc8_claim1": "Leonardo DiCaprio was born on November 11, 1974 in Los Angeles, California.",
      "doc8_claim2": "Leonardo DiCaprio's parents named him after Leonardo da Vinci because his pregnant mother first felt him kick while looking at a da Vinci painting in Florence's Uffizi museum.",
      "doc8_claim3": "Leonardo DiCaprio won his first Academy Award for Best Actor for his role in The Revenant (2015).",
      "doc8_claim4": "Leonardo DiCaprio established the Leonardo DiCaprio Foundation in 1998 to promote environmental awareness.",
      "doc8_claim5": "Leonardo DiCaprio's films have grossed $7.2 billion worldwide.",
      "doc8_claim6": "Leonardo DiCaprio was appointed as United Nations Messenger of Peace in September 2014.",
      "doc8_claim7": "Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3."
    }
  })
```


Generating questions that would require information from different documents can help in constructing multi-hop questions across documents. As can be observed from the outputs, even with the same set of 3 documents, various QA datapoints can be constructed as there are multiple claims from each document, which in turn be permuted in numerous ways. Furthermore, we can construct questions that might require different kinds of reasoning or problem solving capabilities.


|Pattern	|Reasoning Logic	|Example	|
|---	|---	|---	|
|Conjunction	|Claim A Claim B Claim C composite claim.	|
'93Doc 1 says X; Doc 2 says Y; Doc 3 says Z, therefore '85'94	|
|
Comparison	|Claim A vs Claim B contrast.	|
'93'85yet Doc 2 reports a higher value.'94	|
|
Causal chain	|Claim A causes Claim B, which causes Claim C.	|'93'85leading to '85'94	|
|Temporal sequence	|Claim A at t, Claim B at t.	|'93'85after '85'94	|

Each of these would require different prompts and instructions, and are attached below in [Prompts Section](https://quip-amazon.com/aMnQADJbkxmH#temp:C:HDFa0901ef2572849d6bf672f8b2)

#### Examples of Generated QA

#### Temporal

```
[{'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'}],
  'question': "How many years after DiCaprio's film debut did his salary increase to $20 million per movie?",
  'answer': '9 years (from 1991 to 2000)'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim6',
    'claim': 'Leonardo DiCaprio was appointed as United Nations Messenger of Peace in September 2014.'},
   {'doc_id': 7,
    'claim_id': 'doc7_claim5',
    'claim': 'Leonardo DiCaprio received the Actors Inspiration Award at the SAG Foundation 30th-anniversary celebration in November 2015.'}],
  'question': "Which came first: DiCaprio's UN Messenger of Peace appointment or his SAG Foundation Actors Inspiration Award?",
  'answer': 'UN Messenger of Peace appointment (September 2014)'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim4',
    'claim': 'Leonardo DiCaprio established the Leonardo DiCaprio Foundation in 1998 to promote environmental awareness.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim3',
    'claim': 'Leonardo DiCaprio won his first Academy Award for Best Actor for his role in The Revenant (2015).'}],
  'question': 'How many years passed between DiCaprio establishing his environmental foundation and winning his first Oscar?',
  'answer': '17 years (from 1998 to 2015)'}]
```

#### Comparison

```
[{'used_claims': [{'doc_id': 7,
    'claim_id': 'doc7_claim2',
    'claim': 'Leonardo DiCaprio won his first Best Actor Oscar for his role in The Revenant.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'}],
  'question': "How many years passed between Leonardo DiCaprio's film debut and his first Best Actor Oscar win?",
  'answer': '24 years'},
 {'used_claims': [{'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim5',
    'claim': "Leonardo DiCaprio's films have grossed $7.2 billion worldwide."}],
  'question': "What is the ratio between DiCaprio's films' total worldwide gross and his Titanic salary?",
  'answer': '2,880:1'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim2',
    'claim': "Leonardo DiCaprio's parents named him after Leonardo da Vinci because his pregnant mother first felt him kick while looking at a da Vinci painting in Florence's Uffizi museum."},
   {'doc_id': 15,
    'claim_id': 'doc15_claim3',
    'claim': 'Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.'}],
  'question': 'Which museum is specifically named in accounts of how DiCaprio got his name?',
  'answer': 'Uffizi'}]
```

#### Causal

```
[{'used_claims': [{'doc_id': 15,
    'claim_id': 'doc15_claim3',
    'claim': 'Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim2',
    'claim': "Leonardo DiCaprio's parents named him after Leonardo da Vinci because his pregnant mother first felt him kick while looking at a da Vinci painting in Florence's Uffizi museum."},
   {'doc_id': 8,
    'claim_id': 'doc8_claim1',
    'claim': 'Leonardo DiCaprio was born on November 11, 1974 in Los Angeles, California.'}],
  'question': "How did a visit to an Italian museum in 1974 influence a future Hollywood star's identity?",
  'answer': "A mother's experience viewing a da Vinci painting at the Uffizi museum when her baby first kicked led to naming her son Leonardo DiCaprio."},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'}],
  'question': "How did DiCaprio's 1991 horror movie debut eventually impact his earning potential?",
  'answer': 'Starting with a role in Critters 3 led to a career that eventually commanded $20 million per movie by 2000.'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim4',
    'claim': 'Leonardo DiCaprio established the Leonardo DiCaprio Foundation in 1998 to promote environmental awareness.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim1',
    'claim': "Leonardo DiCaprio bought an island in Belize and built the world's first eco-restorative resort that restores the island's over-fished waters, coastline, and forest."}],
  'question': "How did DiCaprio's 1998 environmental initiative evolve into direct ecosystem restoration?",
  'answer': "His foundation's focus on environmental awareness led to him creating an eco-restorative resort in Belize that actively restores marine and forest ecosystems."}]
```

#### Conjunction

```
[{'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim1',
    'claim': 'Leonardo DiCaprio was born on November 11, 1974 in Los Angeles, California.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim3',
    'claim': 'Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.'},
   {'doc_id': 7,
    'claim_id': 'doc7_claim3',
    'claim': "Leonardo DiCaprio's father worked as an underground comic book distributor and his mother was a legal secretary."}],
  'question': "In what year was the legal secretary's son named after seeing a painting in Italy?",
  'answer': '1974'},
 {'used_claims': [{'doc_id': 7,
    'claim_id': 'doc7_claim2',
    'claim': 'Leonardo DiCaprio won his first Best Actor Oscar for his role in The Revenant.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim6',
    'claim': 'Leonardo DiCaprio was appointed as United Nations Messenger of Peace in September 2014.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'}],
  'question': 'Who became a UN Messenger of Peace before winning their first Oscar while commanding $20M per film?',
  'answer': 'Leonardo DiCaprio'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim8',
    'claim': 'Leonardo DiCaprio has never used drugs and required training from drug experts to portray being high.'},
   {'doc_id': 7,
    'claim_id': 'doc7_claim5',
    'claim': 'Leonardo DiCaprio received the Actors Inspiration Award at the SAG Foundation 30th-anniversary celebration in November 2015.'}],
  'question': "Which drug-free actor debuted in Critters 3 and later won the SAG Foundation's Inspiration Award?",
  'answer': 'Leonardo DiCaprio'}]
```

### Observations

* A few comparison questions overlap with temporal questions, for instance '93How many years passed between Leonardo DiCaprio's film debut and his first Best Actor Oscar win?'94. This may be valid as answering such temporal questions could require comparing across two separate documents.
* There are certain questions that don'92t seem to be natural (similar to what real users would actually ask), such as '93What is the ratio between DiCaprio's films' total worldwide gross and his Titanic salary?'94. However, such questions are prevalent in most QA datasets such as [HotPotQA](https://hotpotqa.github.io/), [FreshQA](https://github.com/freshllms/freshqa), [WebQuestions](https://github.com/brmson/dataset-factoid-webquestions?tab=readme-ov-file), ..etc. as their main purpose is to evaluate the capabilities of question answering system.
* Few generated questions have redundant claims that may not have any impact on the answer, such as '93Who became a UN Messenger of Peace before winning their first Oscar while commanding $20M per film?'94. The last part '93while commanding $20M per film'94 is not necessary to arrive at the answer.

#### Prompts Section

(these can be modified based on observations and feedback)

#### Claim Extraction Prompt

```
def extract_standalone_claims_prompt(text):

    PROMPT = f'''
    You are given a passage of text. Your task is to extract one-sentence standalone factual claims from the passage.

    Each claim must:
    - Be self-contained (i.e., understandable on its own without requiring coreference resolution)
    - Not rely on ambiguous references like "this", "that", "they", etc.
    - Be phrased as a complete declarative sentence
    - Be verifiable (e.g., "Riot Games has no plans for League of Legends 2." is acceptable; "They have no plans for League of Legends 2" is not)

    For each claim, also provide the exact verbatim supporting text span from the original text that led you to extract the claim.

    Return your answer in JSON format using the following keys numbered from 1 to n:
    - "claim1": <standalone claim>,
    - "supporting_text_span1": <verbatim supporting text span from original>,
    ...


    Here is the input text:
    {text}
    '''

    return PROMPT
```

#### QA Generation Prompts and Helpers

```
def is_temporal_applicable(*buckets*, *, *min_buckets*=2) -> bool:
    """
    True  at least `min_buckets` buckets each contain 1 explicit year / date
    False otherwise
    """
    buckets_with_dates = sum(
        1 *for* b *in* *buckets* *if* any(_DATE_RX.search(claim) *for* claim *in* b["claims"].values())
    )
    flag = buckets_with_dates >= *min_buckets*
    *return* flag, buckets_with_dates

def build_temporal_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=2):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **temporal-sequence** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Combine claims from at least **{*min_buckets*} different buckets**.  
    '95 Each chosen claim must mention a year, date, or explicit timeframe.  
    '95 Ask for an ordering or an interval (e.g., '93How many years after '85?'94).  
    '95 Every referenced fact must be essential for the answer.
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.  
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide one concise answer (year, number of years, earlier event, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()
*# Comparison QAs*
_NUM_RX = re.compile(r"[,]*(?:.+)?")            *# any numeral*
def is_comparison_applicable(*buckets*, *, *min_buckets*=2) -> bool:
    """
    True  at least `min_buckets` buckets each contain 1 numeric value
    False otherwise
    """
    buckets_with_nums = sum(
        1 *for* b *in* *buckets* *if* any(_NUM_RX.search(claim) *for* claim *in* b["claims"].values())
    )
    flag = buckets_with_nums >= *min_buckets*
    *return* flag, buckets_with_nums

def build_comparison_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=2):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **comparison-style** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Use claims from at least **{*min_buckets*} different buckets**.  
    '95 Frame the question so the solver *must* contrast values  
      (higher, lower, difference, ratio, earlier, later).  
    '95 Keep wording tight'97only include values essential for the comparison.  
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide a single, concise answer (number, entity, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()

*# Causal QAs*
_CAUSE_RX = re.compile(r"(cause[sd]?|lead[s]? to|result(?:ed)? in)", re.I)
def is_causal_applicable(*buckets*, *, *min_buckets*=2) -> bool:
    """
    True  at least `min_buckets` buckets each contain 1 causal phrase
    False otherwise
    """
    buckets_with_cause = sum(
        1 *for* b *in* *buckets* *if* any(_CAUSE_RX.search(claim) *for* claim *in* b["claims"].values())
    )
    flag = buckets_with_cause >= *min_buckets*
    *return* flag, buckets_with_cause

def build_causal_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=2):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **causal-chain** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Pull evidence from at least **{*min_buckets*} different buckets**.  
    '95 Link the facts in an explicit cause-and-effect sequence  
      ('93'85led to'85'94, '93because of'85'94, etc.).  
    '95 Every step must be necessary to reach the answer; omit filler.  
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.  
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide one concise answer (final effect, reason, yes/no, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()
*# Conjunction QAs*
def is_conjunction_applicable(*buckets*, *, *min_buckets*=3) -> bool:
    """
    True  at least `min_buckets` buckets (no other content requirement)
    False otherwise
    """
    flag = len(*buckets*) >= *min_buckets*
    *return* flag, len(*buckets*)

def build_conjunction_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=3):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **conjunction-style** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Combine facts from at least **{*min_buckets*} different buckets** (can use more).  
    '95 The answer must change if *any* referenced claim were false (logical AND).  
    '95 No trivia'97every fact must be essential.  
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.  
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide a single, concise answer (entity, number, date, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()
```




"metadata": {
    "split_name": "high_contamination",
    "model_id": "us.anthropic.claude-sonnet-4-20250514-v1:0",
    "start_index": 0,
    "end_index": 499,
    "total_processed": 500,
    "successful": 498,
    "failed": 2,
    "skipped": 0,
    "total_paraphrase_pairs": 1184,
    "total_processing_time": 2023.7182803153992,
    "paraphrase_percentage": 4.322745527564805,
    "total_possible_pairs": 27390,
    "timestamp": "20250910_183550"
  },
  "percentage_breakdown": {
    "overall_percentage": 4.322745527564805,
    "total_paraphrase_pairs": 1184,
    "total_possible_pairs": 27390,


{
  "metadata": {
    "split_name": "random_split",
    "model_id": "us.anthropic.claude-sonnet-4-20250514-v1:0",
    "start_index": 0,
    "end_index": 499,
    "total_processed": 500,
    "successful": 500,
    "failed": 0,
    "skipped": 0,
    "total_paraphrase_pairs": 1260,
    "total_processing_time": 2084.165546655655,
    "paraphrase_percentage": 4.587489987621058,
    "total_possible_pairs": 27466,
    "timestamp": "20250910_183928"
  },
  "percentage_breakdown": {
    "overall_percentage": 4.587489987621058,
    "total_paraphrase_pairs": 1260,
    "total_possible_pairs": 27466,


def get_paraphrase_checker_prompt(question_set: str) -> str:
    prompt = f'''I will now give you a set of questions. I will enclose a sequential number before each question.
Please help me determine if any of these questions are paraphrases of each other.

CRITICAL: You must evaluate ONLY based on linguistic and structural similarity. Do NOT use any external knowledge about the topics, events, or facts mentioned in the questions. Treat each question as a purely linguistic construct.

Two questions are paraphrases ONLY if they:
1. Ask for the same type of information (e.g., both ask for a number, both ask for a name, both ask for a date)
2. Use different words but convey the same meaning linguistically
3. Have the same grammatical structure and question type
4. Can be understood as asking the same thing without any domain knowledge
5. Are essentially the same question with different wording
6. Have the same key details and context (no additional or different information)

Two questions are NOT paraphrases if they:
1. Ask for different types of information (e.g., one asks for a number, another asks for a name)
2. Have different question types
3. Require external knowledge to determine they refer to the same event or fact
4. Are about different topics, even if they seem related
5. Ask about different aspects of the same topic (e.g., "What happened?" vs "When did it happen?")
6. Have different subjects, objects, or key entities, even if they seem related
7. Include different details, locations, people, or circumstances (e.g., "What happened in Paris?" vs "What happened in London?")
8. Have different temporal references (e.g., "What happened in 2020?" vs "What happened in 2021?")
9. Mention different specific events, even if they seem similar

Examples of what are NOT paraphrases:
- "What is the capital of France?" vs "What is the capital of Germany?" (different countries)
- "How many people live in Paris?" vs "How many people live in London?" (different cities)
- "When did the war start?" vs "When did the war end?" (different time points)
- "What is the population of China?" vs "What is the area of China?" (different measurements)
- "What happened in 2020?" vs "What happened in 2021?" (different years)
- "How many soldiers did the general mobilize?" vs "How many soldiers did the general gather?" (different actions)
- "What happened at the battle of X?" vs "What happened at the battle of Y?" (different battles)
- "How many people died in the earthquake?" vs "How many people died in the flood?" (different disasters)
- "What happened after the attack?" vs "What happened after the attack and the execution?" (different details)
- "What happened in Paris?" vs "What happened in London?" (different locations)
- "How many soldiers were mobilized?" vs "How many soldiers were gathered?" (different actions)

Disregard minor changes in word order, synonyms, and variations in phrasing that don't change the core meaning.

If you find paraphrases, give me a list of tuples, each tuple contains two indices of questions that are paraphrases, along with an explanation of why they are linguistically the same.
If there are no paraphrases, give me an empty list.

Do not include any text outside the JSON response.
The JSON response should be in the following format:
{{
    "paraphrase_index_pairs": [
        [(index1, index2), "explanation of why they are linguistically the same"],
        [(index3, index4), "explanation of why they are linguistically the same"],
        ...
    ]
}}
The indices should be the sequential numbers I gave you before each question.
The explanation should be concise (1-2 sentences) and focus on linguistic similarity only.

Here is the set of questions:
{question_set}

Your response:
    '''
    return prompt








# [Method] **Dynamic Evaluation of WebQA Agents**


We propose a method that utilizes seed graphs that anchor future question generation instead of relying on static datapoints susceptible to memorization or data contamination.
Because every evaluation round renders a fresh QA pair from each graph, no fixed test item can be leaked ahead of time, yet all rounds remain semantically comparable via the common anchors.

### Seed Graphs

At round ***t***, we maintain a set St={G1(t),'85,Gnt(t)} of seed graphs. Each graph encodes the evidence trail (queries, retrieved docs, answer node, etc.) for a distinct information need. To support evolving information, graphs are constructed at evaluation time.

### QA Generation

The generation function Gen takes a seed graph Gi(t)*, ** *a random seed ri,tR, and a fixed generation configuration to produce one QA pair:

(qi,t,ai,t)=Gen(Gi(t);ri,t,),ri,tR                (1)


In equation (1), Gen is an LLM-based generator.bundles the frozen prompt template and all non-random decoding settings (temperature, top-p, stop tokens, etc.) and stays fixed for the entire evaluation round.ri,tis an i.i.d. draw from the base randomness source R; it injects per-sample stochasticity so each call produces a new, fluent question while remaining anchored to the same policy. Holding constant and varying only ri,tensures that every system sees independent but identically distributed QA samples.

### Distributional Consistency

Sampling an independent seed ri,tR for each graph yields the per-graph distribution

Pi,t=Dist(Gen(Gi(t);r,))         (2)

**Joint dataset distribution. **** **Because seeds are independent across graphs, the round-*t* dataset Dt={(q1,t,a1,t),'85,(qnt,t,ant,t)} is an i.i.d. sample from the product measure

Pt=i=1ntPi,t(3)

For a model M under evaluation, let score((q,a),M) be any per-question metric (e.g., exact-match, F1, or log-loss).
Define the aggregate dataset score as the simple average over all ntQA pairs:

Agg(Dt,M)=nt1i=1ntscore((qi,t,ai,t),M)


Because the pairs (qi,t,ai,t) are independent and each (qi,t,ai,t)Pi,t, linearity of expectation gives

EDtPt[Agg(Dt,M)]=nt1i=1ntE(q,a)Pi,t[score((q,a),M)]

Hence the expected accuracy on the entire dataset is simply the average of the per-graph expectations, so every seed graph contributes equally.

### Collision Bound

At evaluation round *t*, we work with the refreshed seed-graph set St={G(t)1,'85,G(t)nt}, and let's assume that for every graph Gi(t)the generator can produce K distinct question'96answer pairs:

Ci,t={(q,a)Pi,t((q,a))>0},Ci,t=K


**Overlap across rounds. **** **For two rounds u<v set Ji,u,v=Ci,uCi,vand let Jmax=maxu<vJi,u,v.
Because answers and evidence evolve, Jmaxis typically small for fast-evolving questions.

**Collision bound.** Sampling one QA pair per round, the probability that seed graph i repeats the exact same pair within ***t*** rounds satisfies

Pr[collision'a0within'a0t]2K2t(t1)Jmax(3)

**Trade-off. **** **To guarantee that this probability is at most a desired threshold (0,1), choose the candidatepool size K such that

K2t(t1)Jmax(4)

Equation(4) captures the trade-offs involved:

* Larger K (i.e., a richer candidate pool) leads to lower collision probability for a fixed number of evaluation rounds t.
* Fewer number of evaluation rounds t allows a smaller K to achieve the same error tolerance .
* Smaller maximum overlap Jmax(i.e., greater drift between graphs) reduces the required value of K.

Because each seed graph is rebuilt from fresh evidence every round, its candidate set changes and Jmaxstays small. Equation 3 quantifies the collision risk, while the design formula (4) provides a direct, tunable guideline for selecting K based on requirements such as the frequency of evaluation rounds.

### Cross-Round Reporting and Compatibility

Because each evaluation round rebuilds the seed-graph set St*,* the underlying product distribution
Pt=iPi,tmay drift over time, especially for graphs corresponding to questions with evolving information.
Consequently, raw accuracies from two different rounds may not be directly comparable. To ensure fair comparisons, the following strategies can be employed.

**i) Snapshot evaluation. **** **Fix a reference round t'86 and freeze its seed graphs** **St'86*****. ** *All systems are then evaluated on the same dataset Dt'86, ensuring identical test conditions.
**ii) Macro-averaged score over rounds. **** **For longitudinal tracking, aggregate over a window {t1,'85,tT} (e.g., the most recent T rounds):
Scoremacro=T1j=1TAgg(Dtj,M) (5)

Each Dtjis an i.i.d. sample from its own distribution Ptj, so (5) summarizes a model'92s average effectiveness under the benchmark'92s natural evolution without letting a single round dominate.


## Empirical Validation

This section aims to demonstrate the following two main claims made in this project:
(i) a static benchmark is vulnerable to leakage, and
(ii) the proposed dynamic protocol eliminates that advantage.

### Experimental Design

**Models.**

* Base model M: An off-the-shelf LLM checkpoint.
* Leaked model Mleak: We continue pre-training M on a static benchmark (such as HotPotQA or FreshQA), intentionally leaking the evaluation data.

**Datasets.**

1. Static benchmark Dstatic: a fixed set of question'96answer pairs.
2. Dynamic dataset rounds Dt1,'85,DtT: we build a fresh evidence graph at round tjand generate one QA pair, following the protocol described in the previous section.

**Metric. **** **We report the aggregate exact-match score Agg(D,M).

### Leakage Effect on the Static Benchmark

aclean=Agg(Dstatic,M),aleak=Agg(Dstatic,Mleak)

Continued pre-training on Dstatic is expected to noticeably inflate performance, in line with previous works on the leakage phenomenon for static datasets.

### Robustness on Dynamic Rounds

The key claim of our benchmark is that '97 even if the entire dataset from one evaluation round is leaked into a model '97 the next round'92s scores remain unaffected because that round is freshly generated from the same per-graph distributions. We verify this claim with a ***Leak&Test*** protocol.

### ***Leak&Test Protocol***

**Step 1: clean evaluation (round** t**).**
(a) Generate the round-t dataset Dt={Gen(Gi(t);ri,t,)}i=1nt
(b) Record the clean model'92s score Agg(Dt,M)

**Step 2: leak. **** **Fine-tune M on Dtto obtain the leaked checkpoint Mtleak.
**Step 3: new evaluation (round t+1).**
(a) Generate a fresh dataset Dt+1={Gen(G(t+1)i;ri,t+1,)}i=1nt+1
(b) Measure the accuracy gap
t+1=Agg(Dt+1,Mtleak)Agg(Dt+1,M)
Iterating this three-step cycle for T evaluation requests yields the independent set of gaps 2,3,'85,T.

### Hypothesis Test

Since random fluctuations can nudge scores up or down, we test whether the {average} leakage gain exceeds a small tolerance (e.g.= 0.02 for a 2-point EM margin).
H0:'a0vs.H1:'a0>,
where =E[].

* **Null (H0).** Any advantage is no larger than the noise threshold ~; leakage is not practically meaningful.
* **Alternative **** **(H1). Leakage yields a systematic gain that exceeds ~.

### Test statistic

We employ a one-sided, one-sample t-test on the (T1) observed gaps. With sample mean and standard deviation scomputed over the (T1) gaps,

t=s/T1.

The one'96sided p-value is obtained by comparing tobsto the Student-t distribution with (T2) degrees of freedom.

### Significance test

At significance level =0.05, reject H0if p<; otherwise conclude that leakage does not confer a practically relevant boost.


