
# [Updates - Aug 6th] **Dynamic Evaluation of WebQA Agents**

Graph representations are ideal for anchoring dynamic test sample generation, as they help in controlling complexity and ensuring consistency across different runs. Previous works show that they are effective for reasoning and KGQA [[1](https://arxiv.org/abs/2309.17167), [2](https://arxiv.org/abs/2503.05049)]. As there are various artifacts that need to be captured when representing the task executions of WebQA agents as graphs, we use the following graph schema.

### Nodes

|Vertex Class	|Label	|Payload	|
|---	|---	|---	|
|Query	|vq	|raw text q	|
|Web Document	|vd	|HTML page	|
|Text Snippet	|vs	|agent observations, thoughts	|
|Answer	|vy*t	|gold answer text	|

### Edges

|Edge Label	|Represents	|
|---	|---	|
|
retrieve	|
document fetched via search or follow-up visit	|
|
evidence	|
snippet extracted; snippet supports answer	|
|
comparison	|
numerical/logical comparison between snippets	|
|
hypothesis	|
agent'92s conjecture as an intermediate snippet	|
|
conclude	|
final reasoning step deriving the answer	|
|
follows	|
snippet generated immediately after another	|

#### Generating Execution Logs

For the 500 questions from FreshQA dataset ([paper](https://arxiv.org/abs/2310.03214), [dataset](https://github.com/freshllms/freshqa)), we use the Perplexica framework ([link](https://github.com/ItzCrazyKns/Perplexica)) to generate raw data. Perplexica operates similarly to the popular agentic search tool [Perplexity](https://www.perplexity.ai/), and uses [SearxNG](https://github.com/searxng/searxng) search engine to augment an LLM of our choice (we use deepseek.r1-v1:0 in our experiments).

#### An example log

`**'question':** 'When did Meta move its headquarters to Austin?'`
`**'answer':** "It hasn't moved to Austin" `
`**'thinking':** Based on the provided context, Meta has NOT moved its headquarters to Austin. While there are discussions about Meta potentially reincorporating in Texas [8][10], a Meta spokesperson explicitly stated that "it does not plan on shifting its corporate headquarters out of Menlo Park, California" [8].fact, Meta has been reducing its presence in Austin by:- Leaving its lease at The Domain [4][9]- Subleasing 120,000 square feet of downtown space [6]'`


```
'sources': [{'id': 1,
   'title': 'Meta in Texas: Downtown Austin renovations, solar power purchase near ...',
   'url': 'https://www.kvue.com/article/money/economy/boomtown-2040/meta-downtown-austin-renovations-solar-bastrop/269-90784b92-0226-4dd6-97ae-9475da00d591',
   'snippet': 'Mar 19, 2025 ... Meta appears to be renovating its Downtown Austin office space. The company also plans to buy all the power from a planned solar farm in...'},
  {'id': 2,
   'title': 'META to move teams to Texas - cbsaustin.com',
   'url': 'https://cbsaustin.com/news/local/meta-to-move-teams-to-texas',
   'snippet': 'AUSTIN, Texas '97 CEO of META, Mark Zuckerberg, announced content oversight teams will be relocating to Texas. The announcement came in a video he posted to Facebook Tuesday morning. "We're moving our trust and safety, and content moderation teams out of California, and our US-based content review ...'},
  {'id': 3,
   'title': "Facebook Moving To Texas? Parent Company 'Meta' Signs Massive Lease In ...",
   'url': 'https://www.dailywire.com/news/facebook-moving-to-texas-parent-company-meta-signs-massive-lease-in-downtown-austin',
   'snippet': 'Facebook's parent company Meta has become the latest California corporation to at least partly move to Texas as it has signed a massive lease called "the largest ever in downtown Austin." "The lease is the largest ever in Downtown Austin and larger than the entire Frost Bank Tower in terms of square feet," KVUE reported.'},
  {'id': 4,
   'title': 'Meta Is Leaving Its Austin Office, Guess What Big Tech Company Is Moving In',
   'url': 'https://finance.yahoo.com/news/meta-leaving-austin-office-guess-211425144.html',
   'snippet': "The office real estate picture in Austin, TX is constantly changing but one company's loss may be another company's gain. Meta (NASDAQ:META) is leaving behind its lease in The Domain, a mixed ..."},
  {'id': 5,
   'title': 'Meta plans $2.9 million renovation in another Texas office space',
   'url': 'https://www.chron.com/culture/article/meta-austin-office-20221773.php',
   'snippet': 'Mar 14, 2025 ... Meta, the parent company of Facebook, Instagram and WhatsApp, is working on a tenant fit-up of a downtown Austin office at 300 West Sixth St.'},
  {'id': 6,
   'title': 'Meta out, IBM in at the Domain | Urbanize Austin',
   'url': 'https://austin.urbanize.city/post/austin-meta-ibm-domain-sublease-cousins',
   'snippet': 'Sep 6, 2024 ... Facebook parent company Meta laid off more than 10,000 employees and announced a 120,000-square-foot sublease in a downtown Austin building in...'},
  {'id': 7,
   'title': 'Offices - Meta',
   'url': 'https://www.meta.com/media-gallery/offices-around-the-world/',
   'snippet': 'Meta headquarters, campuses and offices around the world. Media available for download.'},
  {'id': 8,
   'title': 'Meta in talks to reincorporate in Texas or another state, exit ...',
   'url': 'https://www.reuters.com/technology/meta-talks-reincorporate-texas-or-another-state-exit-delaware-wsj-reports-2025-01-31/',
   'snippet': 'Jan 31, 2025 ... A Meta spokesperson said that it does not plan on shifting its corporate headquarters out of Menlo Park, California, but declined to comment...'},
  {'id': 9,
   'title': "IBM takes over Meta's lease at The Domain in Austin - MySA",
   'url': 'https://www.mysanantonio.com/news/austin/article/meta-ibm-austin-19752226.php',
   'snippet': "Sep 9, 2024 '97 IBM will be taking over Meta's former lease on a 320,000-square foot building at The Domain in North Austin. While some tech companies are ..."},
  {'id': 10,
   'title': "Is Facebook parent-company Meta moving to Texas? Here's what we know.",
   'url': 'https://www.statesman.com/story/business/technology/2025/01/31/facebook-instagram-meta-moving-to-texas/78093842007/',
   'snippet': 'Facebook and Instagram parent company, Meta Platforms, is reportedly in talks to reincorporate in Texas.'},
  {'id': 11,
   'title': "Construction of Meta's data center campus north of Austin to ...",
   'url': 'https://www.bizjournals.com/austin/news/2025/03/26/meta-temple-data-center-construction-update-austin.html',
   'snippet': "Mar 26, 2025 ... The project is part of Meta's growing operations in the Lone Star State. Meta has indicated that it's considering moving its headquarters to..."},
  {'id': 12,
   'title': 'Meta Has Bailed Out of Nearly 1 Million-SF of Austin Office Space',
   'url': 'https://www.connectcre.com/stories/meta-has-bailed-out-of-nearly-1-million-sf-of-office-space-in-austin/',
   'snippet': 'Bailing out on large leases has become all too common of late. Remote work and high interest rates have played havoc within the office sector. Facebook parent company Meta Platforms is shedding another big chunk of Austin office space. It wants out of a 320,000 office building in The Domain in North Austin. The Austin Business Journal ...'},
  {'id': 13,
   'title': 'Meta eyes $2.9M project at downtown office building - Austin ...',
   'url': 'https://www.bizjournals.com/austin/news/2025/03/12/meta-downtown-austin-office.html',
   'snippet': 'Mar 12, 2025 '97 Meta said in June 2023 that it was trying to find a subtenant for the nearly 120,000 square feet it leases in the building because it planned to ...'},
  {'id': 14,
   'title': 'The Facebook Company Is Now Meta | Meta',
   'url': 'https://about.fb.com/news/2021/10/facebook-company-is-now-meta/',
   'snippet': 'Oct 28, 2021 'b7 What is Meta? Meta builds technologies that help people connect, find communities, and grow businesses. When Facebook launched in 2004, it changed the way '85'},
  {'id': 15,
   'title': 'Introducing Oakley Meta Glasses, a New Category of Performance '85',
   'url': 'https://about.fb.com/news/2025/06/introducing-oakley-meta-glasses-a-new-category-of-performance-ai-glasses/',
   'snippet': 'Jun 20, 2025 'b7 Meta and Oakley are collaborating to deliver a brand-new category of Performance AI glasses. Oakley Meta glasses are a new product line that will combine Oakley'92s signature '85'}]}
```

#### Constructing Graphs

Since the agent execution logs are unstructured and presented as agent thoughts, constructing a graph requires parsing the raw text and creating a structured set of steps. An LLM can be used to implement this step, with the following prompt:


```
You are a JSON formatter.

INPUT = (<context> + <question> + <answer>) + TRANSCRIPT

SPLIT RULES
Split TRANSCRIPT into bullets / blank-line paragraphs.
If a bullet has several citations '93[n]'94, create a separate THOUGHT
   for each citation, duplicating the claim text but keeping one '93[n]'94.

STEP-TYPE
conclude  '96 last THOUGHT before the answer OR begins with
              '93overall'94, '93in summary'94, '93therefore'94, '93thus'94, '93hence'94 '85
evidence  '96 contains 1 '93[n]'94 AND no reasoning words (because, implies '85)
reason    '96 all others.
You are a JSON formatter.

INPUT = (<context> + <question> + <answer>) + TRANSCRIPT

Split TRANSCRIPT into bullets / blank-line paragraphs.
If one bullet has several citations '93[n]'94, duplicate it so each
     new bullet carries exactly one '93[n]'94.

For each resulting bullet produce an object
   { "text": <bullet>, "type": <STEP-TYPE> } using:

   STEP-TYPE
     conclude   '96 last bullet before the answer OR starts with
                  '93overall / in summary / therefore / thus / hence'94.
     evidence   '96 contains 1 '93[n]'94 AND no reasoning words
                  (because, implies, therefore, might, etc.).
     retrieve   '96 issues a search or URL visit .
     comparison '96 compares two facts (compared, versus, higher than '85).
     hypothesis '96 speculative statement (maybe, might, could, assume).
     reason     '96 anything else (explanatory but not covered above).

Output a JSON array called **thoughts** '97 no extra keys, no commentary.
OUTPUT FORMAT:

{
  "thoughts":[
    {"id":1,"text":"...","step_type":"evidence|reason|conclude",
     "cites":[{"id":n,"stance":"supports|contradicts|neutral"}]},
    '85
  ],
  "answer":{"text":"...", "cites":[{"id":n,"stance":"..."}]}
}
```


Once the JSON with sequential thoughts and step types is obtained, we can construct graphs such as the ones shown below:
[Image: image.png]

As can be observed, there are several nodes that are not visited. Exploring other parts of the graphs and visiting nodes that were not utilized in answering the original question can help in generating related QA pairs. However, after exploring this approach, a few drawbacks were identified:

* LLM agents make mistakes (expected behavior as performance on FreshQA dataset <80%), and relying on them to solve multi-hop questions may not be the best approach.
* It is non-trivial to localize errors when things go wrong.
* Little control on what claims within a document are utilized for QA generation.


A better approach would be to extract claims within certain documents of interest, and then construct the questions bottom up. This provides higher level of control over the selection of documents, claims, reasoning operations required to answer questions, number of hops and complexity of the question in general.

#### Example

`**FreshQA Question:** What is the name of Leonardo DiCaprio's third child? `
**`FreshQA Answer:`**` Leonardo DiCaprio does not have any children.`

**`Perplexica (w/ DeepSeek-R1) Answer: `**`He has no children. `
`**Sources:** Fetched 15 sources. `
`**Thinking:** Based on the context, Leonardo DiCaprio does not have any children. Multiple sources confirm this, including [1] which states that "kids aren't a priority in his life" and [12] which explicitly states "Leonardo DiCaprio may not have any children."`

Now, to generate related questions (about Leonardo DiCaprio'92s personal life), we can randomly pick a few sources from the retrieved 15 HTML docs. If we were to select 3 documents out of these 15, there would be 15c3 or 455 combinations. While we are using just using top 15 search results for now, this number can be much larger if higher diversity is required.

Next, atomic claims from the long-form text from each document are extracted, similar to FactScore ([link](https://arxiv.org/pdf/2305.14251)).

For example, let'92s assume documents `[7, 15, 8]` were selected. Extracted claims would look like:

Note: The following values are for doc [1,4,8] since I extracted these values after rerunning the cell, but the response for any docs would look similar. The prompt can be found in  [Prompts Section](https://quip-amazon.com/aZgKAJ8xl7nf#temp:C:TdMd87f142d1854483dab70e6996).

```
{1: {'claim1': 'Leonardo DiCaprio has not fathered any children during his time in Hollywood.',
  'supporting_text_span1': 'The rumors have all been false, as Leo has neither been married nor fathered a child since his time in Hollywood.',
  'claim2': 'Leonardo DiCaprio expressed ambiguous thoughts about having children in a 2016 Rolling Stone interview.',
  'supporting_text_span2': '"Do you mean do I want to bring children into a world like this?" he told Rolling Stone in 2016. "If it happens, it happens. I'd prefer not to get into specifics about it, just because then it becomes something that is misquoted. But, yeah. I don't know... To articulate how I feel about it is just gonna be misunderstood,"',
  'claim3': 'Leonardo DiCaprio dated model and actress Camila Morrone from 2017 until their breakup in August 2022.',
  'supporting_text_span3': "Leo's most recent girlfriend, 22-year-old model and actress Camila Morrone, was rumored to be pregnant a few times over the course of their relationship. The pair started dating in 2017, when Camila was only 20 [...] In August 2022, it was reported that he and Camila broke up",
  'claim4': 'Leonardo DiCaprio has been an advocate for environmental causes including clean energy initiatives and climate change policy.',
  'supporting_text_span4': "The actor has advocated for cleaner energy initiatives and immediate policy to reverse the effects of climate change for years. He's continuously used his platform as an actor to promote his ideas."},
 4: {'claim1': 'Leonardo DiCaprio was born on November 11, 1974, in Los Angeles, California.',
  'supporting_text_span1': 'Born on November 11, 1974, in Los Angeles, California',
  'claim2': 'Leonardo DiCaprio was named after the famous painter Leonardo da Vinci.',
  'supporting_text_span2': "DiCaprio was named after the famous painter Leonardo da Vinci, as his mother felt him kick while looking at one of da Vinci's paintings",
  'claim3': 'Leonardo DiCaprio started his acting career at age five in commercials and educational films.',
  'supporting_text_span3': 'DiCaprio started his acting career at the age of five, appearing in commercials and educational films',
  'claim4': 'Leonardo DiCaprio established Appian Way Productions in 2004.',
  'supporting_text_span4': 'In 2004, DiCaprio established his own production company, Appian Way Productions',
  'claim5': "Leonardo DiCaprio won the Academy Award for Best Actor in 2016 for his role in 'The Revenant'.",
  'supporting_text_span5': "DiCaprio finally won the Academy Award for Best Actor in 2016 for his role in 'The Revenant'",
  'claim6': "Leonardo DiCaprio received his first Academy Award nomination for Best Supporting Actor in 1994 for 'What's Eating Gilbert Grape'.",
  'supporting_text_span6': "For his role in 'What's Eating Gilbert Grape,' DiCaprio received his first Academy Award nomination for Best Supporting Actor in 1994",
  'claim7': "Leonardo DiCaprio's first major television role was as Garry Buckman in the sitcom 'Parenthood' in 1990.",
  'supporting_text_span7': "His first major role on television was in the sitcom 'Parenthood' in 1990, where he played Garry Buckman"},
 11: {'claim1': 'Leonardo DiCaprio was born on November 11, 1974, in Los Angeles, California.',
  'supporting_text_span1': 'Leonardo DiCaprio was born on November 11,1974, in Los Angeles, California.',
  'claim2': 'Leonardo DiCaprio is the only child of Irmelin Indenbirken, a legal secretary, and George DiCaprio, an underground comix artist and distributor.',
  'supporting_text_span2': 'Leonardo DiCaprio is the only child of Irmelin Indenbirken, a legal secretary, and George DiCaprio, an underground comix artist and distributor.',
  'claim3': 'Leonardo DiCaprio founded the production company Appian Way Productions in 2004.',
  'supporting_text_span3': 'In 2004, Leonardo DiCaprio founded the production company Appian Way Productions, a namesake of the Italian road.',
  'claim4': 'Leonardo DiCaprio was paid $30 million for his role in Killers of the Flower Moon.',
  'supporting_text_span4': "Leonardo DiCaprio next starred in Scorsese's crime drama Killers of the Flower Moon based on the book of the same name by David Grann , for which he was paid $30 million.",
  'claim5': 'Leonardo DiCaprio was made a commander of the Ordre des Arts et des Lettres by the French Minister of Culture in 2005.',
  'supporting_text_span5': 'In 2005, Leonardo DiCaprio was made a commander of the Ordre des Arts et des Lettres by the French Minister of Culture for his contributions to the arts.'}}
```

As can be seen above, verbatim excerpts from the documents are extracted for each claim. The supporting text spans can be verified by performing simple string match to ensure there are no hallucinations. These text spans are useful later in the LLM-as-a-Judge phase when all generated datapoints are passed through a verification filter.

The claims are then arranged as separate buckets (as shown below) and fed to the LLM generator using prompts presented in [Prompts Section](https://quip-amazon.com/aZgKAJ8xl7nf#temp:C:TdMd87f142d1854483dab70e6996).

```


{
    "doc_id": 7,
    "claims": {
      "doc7_claim1": "Leonardo DiCaprio's childhood photo with his parents has gone viral on social media with over 13 million views.",
      "doc7_claim2": "Leonardo DiCaprio won his first Best Actor Oscar for his role in The Revenant.",
      "doc7_claim3": "Leonardo DiCaprio's father worked as an underground comic book distributor and his mother was a legal secretary.",
      "doc7_claim4": "Leonardo DiCaprio's parents worked for his production company in 1995.",
      "doc7_claim5": "Leonardo DiCaprio received the Actors Inspiration Award at the SAG Foundation 30th-anniversary celebration in November 2015."
    }
  },
  {
    "doc_id": 15,
    "claims": {
      "doc15_claim1": "Leonardo DiCaprio bought an island in Belize and built the world's first eco-restorative resort that restores the island's over-fished waters, coastline, and forest.",
      "doc15_claim2": "Leonardo DiCaprio and Kate Winslet helped pay nursing home fees for the last Titanic survivor to prevent her from selling Titanic mementos.",
      "doc15_claim3": "Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.",
      "doc15_claim4": "Leonardo DiCaprio blocked a film he appeared in from being released in the United States and Canada.",
      "doc15_claim5": "Leonardo DiCaprio turned down the role of Anakin Skywalker in the Star Wars prequels.",
      "doc15_claim6": "Leonardo DiCaprio speaks German fluently due to spending part of his childhood in Germany with his grandparents.",
      "doc15_claim7": "The Taliban banned the Leonardo DiCaprio haircut from Titanic among Afghan youths, with barbers facing jail time for giving it.",
      "doc15_claim8": "Leonardo DiCaprio has never used drugs and required training from drug experts to portray being high.",
      "doc15_claim9": "Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street."
    }
  },
  {
    "doc_id": 8,
    "claims": {
      "doc8_claim1": "Leonardo DiCaprio was born on November 11, 1974 in Los Angeles, California.",
      "doc8_claim2": "Leonardo DiCaprio's parents named him after Leonardo da Vinci because his pregnant mother first felt him kick while looking at a da Vinci painting in Florence's Uffizi museum.",
      "doc8_claim3": "Leonardo DiCaprio won his first Academy Award for Best Actor for his role in The Revenant (2015).",
      "doc8_claim4": "Leonardo DiCaprio established the Leonardo DiCaprio Foundation in 1998 to promote environmental awareness.",
      "doc8_claim5": "Leonardo DiCaprio's films have grossed $7.2 billion worldwide.",
      "doc8_claim6": "Leonardo DiCaprio was appointed as United Nations Messenger of Peace in September 2014.",
      "doc8_claim7": "Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3."
    }
  })
```


Generating questions that would require information from different documents can help in constructing multi-hop questions across documents. As can be observed from the outputs, even with the same set of 3 documents, various QA datapoints can be constructed as there are multiple claims from each document, which in turn be permuted in numerous ways. Furthermore, we can construct questions that might require different kinds of reasoning or problem solving capabilities.


|Pattern	|Reasoning Logic	|Example	|
|---	|---	|---	|
|Conjunction	|Claim A Claim B Claim C composite claim.	|
'93Doc 1 says X; Doc 2 says Y; Doc 3 says Z, therefore '85'94	|
|
Comparison	|Claim A vs Claim B contrast.	|
'93'85yet Doc 2 reports a higher value.'94	|
|
Causal chain	|Claim A causes Claim B, which causes Claim C.	|'93'85leading to '85'94	|
|Temporal sequence	|Claim A at t, Claim B at t.	|'93'85after '85'94	|

Each of these would require different prompts and instructions, and are attached below in [Prompts Section](https://quip-amazon.com/aZgKAJ8xl7nf#temp:C:TdMd87f142d1854483dab70e6996)

#### Examples of Generated QA

#### Temporal

```
[{'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'}],
  'question': "How many years after DiCaprio's film debut did his salary increase to $20 million per movie?",
  'answer': '9 years (from 1991 to 2000)'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim6',
    'claim': 'Leonardo DiCaprio was appointed as United Nations Messenger of Peace in September 2014.'},
   {'doc_id': 7,
    'claim_id': 'doc7_claim5',
    'claim': 'Leonardo DiCaprio received the Actors Inspiration Award at the SAG Foundation 30th-anniversary celebration in November 2015.'}],
  'question': "Which came first: DiCaprio's UN Messenger of Peace appointment or his SAG Foundation Actors Inspiration Award?",
  'answer': 'UN Messenger of Peace appointment (September 2014)'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim4',
    'claim': 'Leonardo DiCaprio established the Leonardo DiCaprio Foundation in 1998 to promote environmental awareness.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim3',
    'claim': 'Leonardo DiCaprio won his first Academy Award for Best Actor for his role in The Revenant (2015).'}],
  'question': 'How many years passed between DiCaprio establishing his environmental foundation and winning his first Oscar?',
  'answer': '17 years (from 1998 to 2015)'}]
```

#### Comparison

```
[{'used_claims': [{'doc_id': 7,
    'claim_id': 'doc7_claim2',
    'claim': 'Leonardo DiCaprio won his first Best Actor Oscar for his role in The Revenant.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'}],
  'question': "How many years passed between Leonardo DiCaprio's film debut and his first Best Actor Oscar win?",
  'answer': '24 years'},
 {'used_claims': [{'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim5',
    'claim': "Leonardo DiCaprio's films have grossed $7.2 billion worldwide."}],
  'question': "What is the ratio between DiCaprio's films' total worldwide gross and his Titanic salary?",
  'answer': '2,880:1'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim2',
    'claim': "Leonardo DiCaprio's parents named him after Leonardo da Vinci because his pregnant mother first felt him kick while looking at a da Vinci painting in Florence's Uffizi museum."},
   {'doc_id': 15,
    'claim_id': 'doc15_claim3',
    'claim': 'Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.'}],
  'question': 'Which museum is specifically named in accounts of how DiCaprio got his name?',
  'answer': 'Uffizi'}]
```

#### Causal

```
[{'used_claims': [{'doc_id': 15,
    'claim_id': 'doc15_claim3',
    'claim': 'Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim2',
    'claim': "Leonardo DiCaprio's parents named him after Leonardo da Vinci because his pregnant mother first felt him kick while looking at a da Vinci painting in Florence's Uffizi museum."},
   {'doc_id': 8,
    'claim_id': 'doc8_claim1',
    'claim': 'Leonardo DiCaprio was born on November 11, 1974 in Los Angeles, California.'}],
  'question': "How did a visit to an Italian museum in 1974 influence a future Hollywood star's identity?",
  'answer': "A mother's experience viewing a da Vinci painting at the Uffizi museum when her baby first kicked led to naming her son Leonardo DiCaprio."},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'}],
  'question': "How did DiCaprio's 1991 horror movie debut eventually impact his earning potential?",
  'answer': 'Starting with a role in Critters 3 led to a career that eventually commanded $20 million per movie by 2000.'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim4',
    'claim': 'Leonardo DiCaprio established the Leonardo DiCaprio Foundation in 1998 to promote environmental awareness.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim1',
    'claim': "Leonardo DiCaprio bought an island in Belize and built the world's first eco-restorative resort that restores the island's over-fished waters, coastline, and forest."}],
  'question': "How did DiCaprio's 1998 environmental initiative evolve into direct ecosystem restoration?",
  'answer': "His foundation's focus on environmental awareness led to him creating an eco-restorative resort in Belize that actively restores marine and forest ecosystems."}]
```

#### Conjunction

```
[{'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim1',
    'claim': 'Leonardo DiCaprio was born on November 11, 1974 in Los Angeles, California.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim3',
    'claim': 'Leonardo DiCaprio received his name because his pregnant mother was looking at a Leonardo da Vinci painting in an Italian museum when he first kicked.'},
   {'doc_id': 7,
    'claim_id': 'doc7_claim3',
    'claim': "Leonardo DiCaprio's father worked as an underground comic book distributor and his mother was a legal secretary."}],
  'question': "In what year was the legal secretary's son named after seeing a painting in Italy?",
  'answer': '1974'},
 {'used_claims': [{'doc_id': 7,
    'claim_id': 'doc7_claim2',
    'claim': 'Leonardo DiCaprio won his first Best Actor Oscar for his role in The Revenant.'},
   {'doc_id': 8,
    'claim_id': 'doc8_claim6',
    'claim': 'Leonardo DiCaprio was appointed as United Nations Messenger of Peace in September 2014.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim9',
    'claim': 'Leonardo DiCaprio earned $2.5 million for Titanic in 1997 and his pay increased to $20 million per movie from 2000 onwards, with exceptions for J Edgar, Django Unchained and The Wolf of Wall Street.'}],
  'question': 'Who became a UN Messenger of Peace before winning their first Oscar while commanding $20M per film?',
  'answer': 'Leonardo DiCaprio'},
 {'used_claims': [{'doc_id': 8,
    'claim_id': 'doc8_claim7',
    'claim': 'Leonardo DiCaprio made his film debut in 1991 in the horror sequel Critters 3.'},
   {'doc_id': 15,
    'claim_id': 'doc15_claim8',
    'claim': 'Leonardo DiCaprio has never used drugs and required training from drug experts to portray being high.'},
   {'doc_id': 7,
    'claim_id': 'doc7_claim5',
    'claim': 'Leonardo DiCaprio received the Actors Inspiration Award at the SAG Foundation 30th-anniversary celebration in November 2015.'}],
  'question': "Which drug-free actor debuted in Critters 3 and later won the SAG Foundation's Inspiration Award?",
  'answer': 'Leonardo DiCaprio'}]
```

### Observations

* A few comparison questions overlap with temporal questions, for instance '93How many years passed between Leonardo DiCaprio's film debut and his first Best Actor Oscar win?'94. This may be valid as answering such temporal questions could require comparing across two separate documents.
* There are certain questions that don'92t seem to be natural (similar to what real users would actually ask), such as '93What is the ratio between DiCaprio's films' total worldwide gross and his Titanic salary?'94. However, such questions are prevalent in most QA datasets such as [HotPotQA](https://hotpotqa.github.io/), [FreshQA](https://github.com/freshllms/freshqa), [WebQuestions](https://github.com/brmson/dataset-factoid-webquestions?tab=readme-ov-file), ..etc. as their main purpose is to evaluate the capabilities of question answering systems (even if they are not realistic or representative of a user).
* Few of the generated questions have redundant claims that may not have any impact on the answer, such as '93Who became a UN Messenger of Peace before winning their first Oscar while commanding $20M per film?'94. The last part '93while commanding $20M per film'94 is not necessary to arrive at the answer. These can be filtered in the next phase using LLM judges.



### To do:

* Implement LLM-as-a-Judge component with 3 different models (selection TBD)
* Prepare midpoint presentation (pipeline diagrams, plots, ..etc.)
* Perform Leak&Test experiments (as described in [[Method] Dynamic Evaluation of WebQA Agents](https://quip-amazon.com/b2HgAfPdfFlm))



### Prompts Section

(these can be modified based on observations and feedback)

#### Claim Extraction Prompt

```
def extract_standalone_claims_prompt(text):

    PROMPT = f'''
    You are given a passage of text. Your task is to extract one-sentence standalone factual claims from the passage.

    Each claim must:
    - Be self-contained (i.e., understandable on its own without requiring coreference resolution)
    - Not rely on ambiguous references like "this", "that", "they", etc.
    - Be phrased as a complete declarative sentence
    - Be verifiable (e.g., "Riot Games has no plans for League of Legends 2." is acceptable; "They have no plans for League of Legends 2" is not)

    For each claim, also provide the exact verbatim supporting text span from the original text that led you to extract the claim.

    Return your answer in JSON format using the following keys numbered from 1 to n:
    - "claim1": <standalone claim>,
    - "supporting_text_span1": <verbatim supporting text span from original>,
    ...


    Here is the input text:
    {text}
    '''

    return PROMPT
```

#### QA Generation Prompts and Helpers

```
def is_temporal_applicable(*buckets*, *, *min_buckets*=2) -> bool:
    """
    True  at least `min_buckets` buckets each contain 1 explicit year / date
    False otherwise
    """
    buckets_with_dates = sum(
        1 *for* b *in* *buckets* *if* any(_DATE_RX.search(claim) *for* claim *in* b["claims"].values())
    )
    flag = buckets_with_dates >= *min_buckets*
    *return* flag, buckets_with_dates

def build_temporal_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=2):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **temporal-sequence** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Combine claims from at least **{*min_buckets*} different buckets**.  
    '95 Each chosen claim must mention a year, date, or explicit timeframe.  
    '95 Ask for an ordering or an interval (e.g., '93How many years after '85?'94).  
    '95 Every referenced fact must be essential for the answer.
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.  
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide one concise answer (year, number of years, earlier event, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()
*# Comparison QAs*
_NUM_RX = re.compile(r"[,]*(?:.+)?")            *# any numeral*
def is_comparison_applicable(*buckets*, *, *min_buckets*=2) -> bool:
    """
    True  at least `min_buckets` buckets each contain 1 numeric value
    False otherwise
    """
    buckets_with_nums = sum(
        1 *for* b *in* *buckets* *if* any(_NUM_RX.search(claim) *for* claim *in* b["claims"].values())
    )
    flag = buckets_with_nums >= *min_buckets*
    *return* flag, buckets_with_nums

def build_comparison_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=2):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **comparison-style** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Use claims from at least **{*min_buckets*} different buckets**.  
    '95 Frame the question so the solver *must* contrast values  
      (higher, lower, difference, ratio, earlier, later).  
    '95 Keep wording tight'97only include values essential for the comparison.  
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide a single, concise answer (number, entity, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()

*# Causal QAs*
_CAUSE_RX = re.compile(r"(cause[sd]?|lead[s]? to|result(?:ed)? in)", re.I)
def is_causal_applicable(*buckets*, *, *min_buckets*=2) -> bool:
    """
    True  at least `min_buckets` buckets each contain 1 causal phrase
    False otherwise
    """
    buckets_with_cause = sum(
        1 *for* b *in* *buckets* *if* any(_CAUSE_RX.search(claim) *for* claim *in* b["claims"].values())
    )
    flag = buckets_with_cause >= *min_buckets*
    *return* flag, buckets_with_cause

def build_causal_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=2):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **causal-chain** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Pull evidence from at least **{*min_buckets*} different buckets**.  
    '95 Link the facts in an explicit cause-and-effect sequence  
      ('93'85led to'85'94, '93because of'85'94, etc.).  
    '95 Every step must be necessary to reach the answer; omit filler.  
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.  
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide one concise answer (final effect, reason, yes/no, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()
*# Conjunction QAs*
def is_conjunction_applicable(*buckets*, *, *min_buckets*=3) -> bool:
    """
    True  at least `min_buckets` buckets (no other content requirement)
    False otherwise
    """
    flag = len(*buckets*) >= *min_buckets*
    *return* flag, len(*buckets*)

def build_conjunction_prompt(*buckets*, *, *n_pairs*=3, *min_buckets*=3):
    *return* textwrap.dedent(f"""
    You are generating *{*n_pairs*}* **conjunction-style** multi-hop QA pairs.
    Buckets of claims (grouped by doc_id) are below:
    {json.dumps(*buckets*, *indent*=2)}
    **Rules for each QA pair**
    '95 Combine facts from at least **{*min_buckets*} different buckets** (can use more).  
    '95 The answer must change if *any* referenced claim were false (logical AND).  
    '95 No trivia'97every fact must be essential.  
    '95 The question should not contain the answer or steps to get the answer.
    '95 The question should not refer to the documents, they should be general.  
    '95 List every claim you used by its doc_id and claim_id.  
    '95 Provide a single, concise answer (entity, number, date, etc.).  
    **Output format (JSON list)**
    - "used_claims": list of {{doc_id, claim_id, claim}}
    - "question":   string
    - "answer":     string
    Return only the JSON'97no extra commentary.
    """).strip()
```




